{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So can get yadlt\n",
    "import sys\n",
    "sys.path.append('/usr2/mamille2/nn4nlp-scripts/mm_dbm/Deep-Learning-TensorFlow/')\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import inspect\n",
    "\n",
    "import dbm\n",
    "import config\n",
    "import rbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "\n",
    "# for f in ['train', 'valid']:\n",
    "for f in ['train']:\n",
    "    dataset[f] = np.load(\"/usr2/mamille2/twitter/data/huang2016_data/tags_{}.npy\".format(f))\n",
    "\n",
    "# D = np.sum(np.vstack((dataset['train'], dataset['valid'])), axis=1)  # length of each document\n",
    "D = np.sum(dataset['train'], axis=1)  # length of each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating /usr2/mamille2/twitter/data/mm_dbm/model/stored_models/ directory to save/restore models\n",
      "Creating /usr2/mamille2/twitter/data/mm_dbm/model/data/ directory to save model generated data\n",
      "Creating /usr2/mamille2/twitter/data/mm_dbm/model/logs/ directory to save tensorboard logs\n",
      "Creating /usr2/mamille2/twitter/data/mm_dbm/model/stored_models/rbm-1 directory to save/restore models\n",
      "Creating /usr2/mamille2/twitter/data/mm_dbm/model/data/rbm-1 directory to save model generated data\n",
      "Creating /usr2/mamille2/twitter/data/mm_dbm/model/logs/rbm-1 directory to save tensorboard logs\n",
      "Creating /usr2/mamille2/twitter/data/mm_dbm/model/stored_models/rbm-2 directory to save/restore models\n",
      "Creating /usr2/mamille2/twitter/data/mm_dbm/model/data/rbm-2 directory to save model generated data\n",
      "Creating /usr2/mamille2/twitter/data/mm_dbm/model/logs/rbm-2 directory to save tensorboard logs\n"
     ]
    }
   ],
   "source": [
    "model = dbm.DBM(\n",
    "    main_dir=\"/usr2/mamille2/twitter/data/huang2016_data/twitter_tags_rbm\", do_pretrain=True, layers=[1024, 1024],\n",
    "    models_dir=config.models_dir, data_dir=config.data_dir, summary_dir=config.summary_dir,\n",
    "    learning_rate=[0.001, 0.001], momentum=0.9, num_epochs=[1, 1], batch_size=[64, 64],\n",
    "    stddev=0.1, verbose=1, gibbs_k=[1, 1], model_name=\"twitter_tags_dbm\",\n",
    "    finetune_learning_rate=0.01, finetune_enc_act_func=[tf.nn.sigmoid, tf.nn.sigmoid],\n",
    "    finetune_dec_act_func=[tf.nn.sigmoid, tf.nn.sigmoid], finetune_num_epochs=1, finetune_batch_size=128,\n",
    "    finetune_opt='momentum', finetune_loss_func=\"mean_squared\", finetune_dropout=0.5, noise=[\"rsm\", \"bin\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize documents lengths\n",
    "for i, _ in enumerate(model.rbms):\n",
    "    model.rbms[i].D = D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215118\n",
      "26039\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset['train']))\n",
    "print(len(dataset['valid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer 1...\n",
      "Tensorboard logs dir for this run is /usr2/mamille2/twitter/data/mm_dbm/model/logs/rbm-1/run10\n",
      "Reconstruction loss at step 0: 0.012994\n",
      "Training layer 2...\n",
      "Tensorboard logs dir for this run is /usr2/mamille2/twitter/data/mm_dbm/model/logs/rbm-2/run10\n",
      "Reconstruction loss at step 0: 0.0155692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 0.78262526,  0.24725132,  0.2698831 , ...,  0.39486814,\n",
       "          0.45765743,  0.03806502],\n",
       "        [ 0.78262526,  0.24725132,  0.2698831 , ...,  0.39486814,\n",
       "          0.45765743,  0.03806502],\n",
       "        [ 0.77756256,  0.24815096,  0.28044698, ...,  0.39678276,\n",
       "          0.44103429,  0.03584903],\n",
       "        ..., \n",
       "        [ 0.78261638,  0.24571618,  0.27131805, ...,  0.43171808,\n",
       "          0.44519871,  0.04281552],\n",
       "        [ 0.78262526,  0.24725132,  0.2698831 , ...,  0.39486814,\n",
       "          0.45765743,  0.03806502],\n",
       "        [ 0.78262526,  0.24725132,  0.2698831 , ...,  0.39486814,\n",
       "          0.45765743,  0.03806502]], dtype=float32),\n",
       " array([[ 0.78535336,  0.26702645,  0.27771628, ...,  0.34255692,\n",
       "          0.43727925,  0.03904784],\n",
       "        [ 0.78262526,  0.24725132,  0.2698831 , ...,  0.39486814,\n",
       "          0.45765743,  0.03806502],\n",
       "        [ 0.78703976,  0.27790949,  0.30201226, ...,  0.38000956,\n",
       "          0.43650389,  0.03872642],\n",
       "        ..., \n",
       "        [ 0.78262526,  0.24725132,  0.2698831 , ...,  0.39486814,\n",
       "          0.45765743,  0.03806502],\n",
       "        [ 0.77747357,  0.23144448,  0.27043855, ...,  0.41009226,\n",
       "          0.44381827,  0.03617172],\n",
       "        [ 0.78262526,  0.24725132,  0.2698831 , ...,  0.39486814,\n",
       "          0.45765743,  0.03806502]], dtype=float32))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pretraining phase \n",
    "# model.pretrain(dataset['train'][:500], dataset['valid'][500:1000])\n",
    "model.pretrain(dataset['train'][:10000], dataset['train'][10000:20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorboard logs dir for this run is /usr2/mamille2/twitter/data/mm_dbm/model/logs/run9\n",
      "Reconstruction loss at step 0: 0.430305\n"
     ]
    }
   ],
   "source": [
    "# Fit and save the txt-DBM parameters \n",
    "# I put save_dbm_text_params as a quick hack to save the parameters of this dbm as a numpy array\n",
    "\n",
    "# takes args train_set, train_ref, validation_set, validation_ref -- unsupervised, so just use more of the dataset\n",
    "\n",
    "train_split = len(dataset['train'])//4\n",
    "# valid_split = len(dataset['valid'])//4\n",
    "\n",
    "# model.fit(dataset['train'][:500], dataset['train'][500:1000], dataset['valid'][:500], dataset['valid'][500:1000])\n",
    "# model.fit(dataset['train'][:train_split], dataset['train'][train_split:], dataset['valid'][:valid_split], dataset['valid'][valid_split:-1])\n",
    "model.fit(dataset['train'][:train_split], dataset['train'][train_split:2*train_split], \n",
    "          dataset['train'][2*train_split:3*train_split], dataset['train'][3*train_split:-2])\n",
    "# model.fit(dataset['train'][:1000], dataset['train'][1000:2000], dataset['valid'][:500], dataset['valid'][500:1000])\n",
    "# model.fit(dataset['train'][:1000], dataset['train'][1000:2000], dataset['valid'][:1000], dataset['valid'][1000:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save out model params\n",
    "outpath = '/usr2/mamille2/twitter/data/mm_dbm/model/tag_dbms_layer_{}_{}.npy'\n",
    "\n",
    "for i in range(len(model.rbms)):\n",
    "    for p in ['W', 'bh_', 'bv_']: # not sure whether uses bh or bv\n",
    "        np.save(outpath.format(i, p), model.rbms[0].get_model_parameters(graph=model.rbm_graphs[0])[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1024)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.rbms[0].get_model_parameters(graph=model.rbm_graphs[0])['W'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 1024)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.rbms[1].get_model_parameters(graph=model.rbm_graphs[1])['W'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.rbms[0].get_model_parameters(graph=model.rbm_graphs[0])['bh_'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.rbms[0].get_model_parameters(graph=model.rbm_graphs[0])['bv_'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format tweet hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet_id', 'created_at', 'in_reply_to_status_id', 'lang',\n",
       "       'retweet_count', 'user_id', 'user_screen_name', 'user_name', 'text',\n",
       "       'text_no_tags', 'tags'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {}\n",
    "\n",
    "for f in ['train', 'valid', 'test']:\n",
    "    data[f] = pd.read_pickle('/usr2/mamille2/twitter/data/huang2016_data/huang2016_{}.pkl'.format(f))\n",
    "    \n",
    "data['train'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3890"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tag vocabulary\n",
    "\n",
    "with open('/usr2/mamille2/twitter/data/huang2016_data/hashtag_vocab.txt') as f:\n",
    "    tags = f.read().splitlines()\n",
    "\n",
    "tag_vocab_len = len(tags)\n",
    "tag_vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3890"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create vocab indices dict\n",
    "tag2idx = defaultdict(int)\n",
    "\n",
    "for t in tags:\n",
    "    tag2idx[t] = tags.index(t)\n",
    "    \n",
    "len(tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def tags2indices(tags, max_len):\n",
    "def tags2indices(tags):\n",
    "    \"\"\" Converts tags to indices \"\"\"\n",
    "    inds = [tag2idx[t] for t in tags]\n",
    "#     padding = [0] * (max_len - len(tags))\n",
    "    return inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert from tag indices to one-hot vectors\n",
    "def inds2hot(tag_inds, len_vocab):\n",
    "    \"\"\" Converts tags from indices to one-hot vectors \"\"\"\n",
    "    \n",
    "    onehot = np.zeros(len_vocab)\n",
    "    for l in tag_inds:\n",
    "        onehot[l] = 1\n",
    "        \n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Covert from tags to one-hot vectors\n",
    "def tags2hot(tags, vocab_len):\n",
    "    return inds2hot(tags2indices(tags), vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert dataset to tag indices\n",
    "tag_vecs = {}\n",
    "\n",
    "for f in ['train', 'valid', 'test']:\n",
    "    tag_vecs[f] = np.array(data[f]['tags'].map(lambda x: tags2hot(x, tag_vocab_len)).tolist())\n",
    "    \n",
    "    np.save('/usr2/mamille2/twitter/data/huang2016_data/tags_{}.npy'.format(f), tag_vecs[f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_vecs['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37644"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check tag vocabulary\n",
    "vocab = set([t for l in data['train']['tags'].tolist() for t in l])\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find max number of tags\n",
    "len_tags = train_data['tags'].map(lambda x: len(x))\n",
    "max(len_tags)\n",
    "\n",
    "# Find max number of tags\n",
    "len_tags = valid_data['tags'].map(lambda x: len(x))\n",
    "max(len_tags)\n",
    "\n",
    "# Find max number of tags\n",
    "len_tags = test_data['tags'].map(lambda x: len(x))\n",
    "max(len_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = tags2indices(['coaching', 'walking'], MAX_LEN)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testvec = inds2hot(test, tag_vocab_len)\n",
    "testvec[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
