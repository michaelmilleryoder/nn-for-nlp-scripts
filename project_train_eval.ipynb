{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr0/home/mamille2/anaconda3/bin:/usr/local/cuda/bin:/usr/local/cuda/lib64:/usr/local/cuda/lib64/:/usr0/home/mamille2/anaconda3/bin:/usr0/home/mamille2/anaconda3/bin:/usr/cs/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr0/home/mamille2/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:32: DeprecationWarning: `Tracer` is deprecated since version 5.1, directly use `IPython.core.debugger.Pdb.set_trace()`\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import spacy\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from time import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import string\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle\n",
    "\n",
    "import dynet_config\n",
    "dynet_config.set(mem=8192, random_seed=12345, autobatch=True) # was 2048 mem\n",
    "dynet_config.set_gpu()\n",
    "\n",
    "import dynet as dy\n",
    "\n",
    "dyparams = dy.DynetParams()\n",
    "dyparams.init()\n",
    "dyparams.set_requested_gpus(1)\n",
    "\n",
    "from IPython.core.debugger import Tracer; debug_here = Tracer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spaCy\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading spaCy\")\n",
    "nlp = spacy.load('en')\n",
    "assert nlp.path is not None\n",
    "print ('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 100\n",
    "NUM_TAGS = 3883\n",
    "# VOCAB_CAP = 10000\n",
    "VOCAB_CAP = 50000\n",
    "\n",
    "UNK = '<UNK>'\n",
    "START = '<S>'\n",
    "END = '</S>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data for Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('/usr0/home/mamille2/twitter/data/huang2016/huang2016_train.aligned.pkl', 'rb') as f:\n",
    "#     twitter_texts, twitter_tags, twitter_histories = pickle.load(f)\n",
    "    twitter_texts, twitter_tags, _ = pickle.load(f)\n",
    "#     , twitter_tags, _ = pickle.load(f)\n",
    "    \n",
    "with open('/usr0/home/mamille2/twitter/data/huang2016/huang2016_valid.aligned.pkl', 'rb') as f:\n",
    "#     dev_texts, dev_tags, dev_histories = pickle.load(f)\n",
    "    dev_texts, dev_tags, _ = pickle.load(f)\n",
    "#     _, dev_tags, _ = pickle.load(f)\n",
    "    \n",
    "with open('/usr0/home/mamille2/twitter/data/huang2016/huang2016_test.aligned.pkl', 'rb') as f:\n",
    "#     _, test_tags, _ = pickle.load(f)\n",
    "    test_texts, test_tags, _ = pickle.load(f)\n",
    "    \n",
    "# del twitter_histories, dev_histories, test_histories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_tags(tags_list, tag_set, tag_dict):\n",
    "    return [[tag_dict[tag] for tag in tags if tag in tag_set] for tags in tags_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212855\n",
      "25817\n",
      "19614\n"
     ]
    }
   ],
   "source": [
    "print(len(twitter_texts))\n",
    "print(len(dev_texts))\n",
    "print(len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37399\n",
      "3883 unique tags.\n"
     ]
    }
   ],
   "source": [
    "# Extract tag set\n",
    "tag_counts = defaultdict(int)\n",
    "for t in twitter_tags:\n",
    "    for x in t:\n",
    "        tag_counts[x] += 1\n",
    "print(len(tag_counts))\n",
    "top_k_tags = set(sorted(tag_counts, key=tag_counts.get, reverse=True)[:NUM_TAGS])\n",
    "\n",
    "tag_set = set()\n",
    "for t in twitter_tags:\n",
    "    tag_set.update(set([x for x in t if x in top_k_tags]))\n",
    "    \n",
    "tag_set = sorted(tag_set)\n",
    "print ('{} unique tags.'.format(len(tag_set)))\n",
    "\n",
    "tag_indexes = defaultdict(lambda: len(tag_indexes))\n",
    "parsed_tags = index_tags(twitter_tags, tag_set, tag_indexes)\n",
    "idx_to_tag = {v: k for k, v in tag_indexes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to open preprecessed TRAIN data ... DONE. (1.752s)\n",
      "Vocab size: 50000\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print ('Attempting to open preprecessed TRAIN data ... ', end='')\n",
    "#     raise NotImplemented\n",
    "    \n",
    "    t0=time()\n",
    "    with open('/usr0/home/mamille2/twitter/data/huang2016/parsed_twitter_train_data_no_histories.pkl', 'rb') as f:\n",
    "        vocab, parsed_texts, parsed_tags = pickle.load(f)\n",
    "    print ('DONE. ({:.3f}s)'.format(time()-t0))\n",
    "        \n",
    "except:\n",
    "    print ('FAIL.')\n",
    "    \n",
    "    print ('\\tParsing texts ... ', end='')\n",
    "    t0=time()\n",
    "    parsed_texts = [[str(w) for w in t][:MAX_LEN] for t in nlp.pipe([x.encode('ascii', 'ignore').decode('ascii').lower() for x in twitter_texts], n_threads=3, batch_size=20000)]\n",
    "    print ('DONE. ({:.3f}s)'.format(time()-t0))\n",
    "    \n",
    "    print ('\\tCounting words ... ', end='')\n",
    "    word_counts = defaultdict(int)\n",
    "    for t in parsed_texts:\n",
    "        for x in t:\n",
    "            word_counts[x] += 1\n",
    "    top_k_words = set(sorted(word_counts, key=word_counts.get, reverse=True)[:VOCAB_CAP-3])\n",
    "\n",
    "    word_set = set()\n",
    "    for t in parsed_texts:\n",
    "        word_set.update(set([x for x in t if x in top_k_words]))\n",
    "    print ('DONE. ({:.3f}s)'.format(time()-t0)) \n",
    "    \n",
    "    vocab = defaultdict(lambda: len(vocab))\n",
    "    print ('\\tIndexing texts ... ', end='')\n",
    "    t0=time()\n",
    "    parsed_texts = [[vocab[START]] + [(vocab[w] if w in word_set else vocab[UNK]) for w in t] + [vocab[END]] for t in parsed_texts]\n",
    "    print ('DONE. ({:.3f}s)'.format(time()-t0))\n",
    "    \n",
    "    unk_idx = vocab[UNK]\n",
    "    sos_idx = vocab[START]\n",
    "    eos_idx = vocab[END]\n",
    "    \n",
    "    print ('\\tSAVING parsed data ... ', end='')\n",
    "    t0=time()\n",
    "    with open('parsed_twitter_train_data_no_histories.pkl', 'wb') as f:\n",
    "        pickle.dump((dict(vocab), parsed_texts, parsed_tags), f) \n",
    "    print ('DONE. ({:.3f}s)'.format(time()-t0))\n",
    "\n",
    "unk_idx = vocab[UNK]\n",
    "sos_idx = vocab[START]\n",
    "eos_idx = vocab[END]\n",
    "# Set unknown words to be UNK --> note as written, the paper does not indicate that any training data is labeled as UNK...\n",
    "vocab = defaultdict(lambda: unk_idx, vocab)\n",
    "idx_to_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "print ('Vocab size:', VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3883"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of tags\n",
    "tagc = Counter([t for tags in parsed_tags for t in tags])\n",
    "len(tagc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to open preprecessed DEV and TEST data ... DONE. (0.066s)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print ('Attempting to open preprecessed DEV and TEST data ... ', end='')\n",
    "#     raise NotImplemented\n",
    "    \n",
    "    t0=time()\n",
    "    with open('/usr0/home/mamille2/twitter/data/huang2016/parsed_twitter_test_dev_data_no_histories.pkl', 'rb') as f:\n",
    "        parsed_dev_texts, parsed_test_texts = pickle.load(f)\n",
    "    print ('DONE. ({:.3f}s)'.format(time()-t0))\n",
    "        \n",
    "except:\n",
    "    print ('FAIL.')\n",
    "    print ('\\tParsing texts ... ', end='')\n",
    "    t0=time()\n",
    "    parsed_dev_texts = [[vocab[START]] + [vocab[str(w)] for w in t if not w.is_stop][:MAX_LEN] + [vocab[END]] for t in nlp.pipe([x.encode('ascii', 'ignore').decode('ascii').lower() for x in dev_texts], n_threads=3, batch_size=20000)]\n",
    "    parsed_test_texts = [[vocab[START]] + [vocab[str(w)] for w in t if not w.is_stop][:MAX_LEN] + [vocab[END]] for t in nlp.pipe([x.encode('ascii', 'ignore').decode('ascii').lower() for x in test_texts], n_threads=3, batch_size=20000)]\n",
    "    print ('DONE. ({:.3f}s)'.format(time()-t0))\n",
    "    \n",
    "    print ('\\tSAVING parsed data ... ', end='')\n",
    "    t0=time()\n",
    "    with open('parsed_twitter_test_dev_data_no_histories.pkl', 'wb') as f:\n",
    "        pickle.dump((parsed_dev_texts, parsed_test_texts), f) \n",
    "    print ('DONE. ({:.3f}s)'.format(time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list(zip(parsed_texts, parsed_tags))\n",
    "dev_tags = index_tags(dev_tags, tag_set, tag_indexes)\n",
    "dev = list(zip(parsed_dev_texts, dev_tags))\n",
    "test_tags = index_tags(test_tags, tag_set, tag_indexes)\n",
    "test = list(zip(parsed_test_texts, test_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "# HIDDEN_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "Q_DIM = 512\n",
    "DROPOUT = 0.2\n",
    "# DROPOUT = 0\n",
    "ALPHA = 0.01\n",
    "EPSILON_MAX = .9\n",
    "EPSILON_MIN = 0.00\n",
    "KL_WEIGHT_START = 0.0\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "PATIENCE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dynet model\n",
    "model = dy.ParameterCollection()\n",
    "\n",
    "# The paper uses AdaGrad\n",
    "trainer = dy.AdamTrainer(model)\n",
    "\n",
    "# Embedding parameters\n",
    "embed = model.add_lookup_parameters((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "# Recurrent layers for tweet encoding\n",
    "lstm_encode = dy.LSTMBuilder(1, EMBEDDING_DIM, HIDDEN_DIM, model)\n",
    "lstm_decode = dy.LSTMBuilder(1, EMBEDDING_DIM, Q_DIM, model)\n",
    "\n",
    "# Encoder MLP for tweet encoding\n",
    "W_mu_tweet_p = model.add_parameters((Q_DIM, HIDDEN_DIM))\n",
    "V_mu_tweet_p = model.add_parameters((HIDDEN_DIM, Q_DIM))\n",
    "b_mu_tweet_p = model.add_parameters((Q_DIM))\n",
    "\n",
    "W_sig_tweet_p = model.add_parameters((Q_DIM, HIDDEN_DIM))\n",
    "V_sig_tweet_p = model.add_parameters((HIDDEN_DIM, Q_DIM))\n",
    "b_sig_tweet_p = model.add_parameters((Q_DIM))\n",
    "\n",
    "W_mu_tag_p = model.add_parameters((Q_DIM, NUM_TAGS))\n",
    "V_mu_tag_p = model.add_parameters((HIDDEN_DIM, Q_DIM))\n",
    "b_mu_tag_p = model.add_parameters((Q_DIM))\n",
    "\n",
    "W_sig_tag_p = model.add_parameters((Q_DIM, NUM_TAGS))\n",
    "V_sig_tag_p = model.add_parameters((HIDDEN_DIM, Q_DIM))\n",
    "b_sig_tag_p = model.add_parameters((Q_DIM))\n",
    "\n",
    "W_mu_p = model.add_parameters((Q_DIM, 2 * HIDDEN_DIM))\n",
    "b_mu_p = model.add_parameters((Q_DIM))\n",
    "\n",
    "W_sig_p = model.add_parameters((Q_DIM, 2 * HIDDEN_DIM))\n",
    "b_sig_p = model.add_parameters((Q_DIM))\n",
    "\n",
    "W_hidden_p = model.add_parameters((HIDDEN_DIM, Q_DIM))\n",
    "b_hidden_p = model.add_parameters((HIDDEN_DIM))\n",
    "\n",
    "W_tweet_softmax_p = model.add_parameters((VOCAB_SIZE, Q_DIM))\n",
    "b_tweet_softmax_p = model.add_parameters((VOCAB_SIZE))\n",
    "\n",
    "W_tag_output_p = model.add_parameters((NUM_TAGS, HIDDEN_DIM))\n",
    "b_tag_output_p = model.add_parameters((NUM_TAGS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mu, log_sigma_squared):\n",
    "    d = mu.dim()[0][0]\n",
    "    sample = dy.random_normal(d)\n",
    "    covar = dy.exp(log_sigma_squared * 0.5)\n",
    "\n",
    "    return mu + dy.cmult(covar, sample)\n",
    "\n",
    "def mlp(x, W, V, b):\n",
    "    return V * dy.tanh(W * x + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(sent, epsilon=0.0):\n",
    "    #dy.renew_cg()\n",
    "    \n",
    "    # Transduce all batch elements with an LSTM\n",
    "    src = sent[0]\n",
    "    tags = sent[1]\n",
    "\n",
    "    # initialize the LSTM\n",
    "    init_state_src = lstm_encode.initial_state()\n",
    "\n",
    "    # get the output of the first LSTM\n",
    "    src_output = init_state_src.add_inputs([embed[x] for x in src])[-1].output()\n",
    "\n",
    "    # Now compute mean and standard deviation of source hidden state.\n",
    "    W_mu_tweet = dy.parameter(W_mu_tweet_p)\n",
    "    V_mu_tweet = dy.parameter(V_mu_tweet_p)\n",
    "    b_mu_tweet = dy.parameter(b_mu_tweet_p)\n",
    "\n",
    "    W_sig_tweet = dy.parameter(W_sig_tweet_p)\n",
    "    V_sig_tweet = dy.parameter(V_sig_tweet_p)\n",
    "    b_sig_tweet = dy.parameter(b_sig_tweet_p)\n",
    "    \n",
    "    # Compute tweet encoding\n",
    "    mu_tweet      = dy.dropout(mlp(src_output, W_mu_tweet,  V_mu_tweet,  b_mu_tweet), DROPOUT)\n",
    "    log_var_tweet = dy.dropout(mlp(src_output, W_sig_tweet, V_sig_tweet, b_sig_tweet), DROPOUT)\n",
    "    \n",
    "    W_mu_tag = dy.parameter(W_mu_tag_p)\n",
    "    V_mu_tag = dy.parameter(V_mu_tag_p)\n",
    "    b_mu_tag = dy.parameter(b_mu_tag_p)\n",
    "\n",
    "    W_sig_tag = dy.parameter(W_sig_tag_p)\n",
    "    V_sig_tag = dy.parameter(V_sig_tag_p)\n",
    "    b_sig_tag = dy.parameter(b_sig_tag_p)\n",
    "    \n",
    "    # Compute tag encoding\n",
    "    tags_tensor = dy.sparse_inputTensor([tags], np.ones((len(tags),)), (NUM_TAGS,))\n",
    "    \n",
    "    mu_tag      = dy.dropout(mlp(tags_tensor, W_mu_tag,  V_mu_tag,  b_mu_tag), DROPOUT)\n",
    "    log_var_tag = dy.dropout(mlp(tags_tensor, W_sig_tag, V_sig_tag, b_sig_tag), DROPOUT)\n",
    "    \n",
    "    # Combine encodings for mean and diagonal covariance\n",
    "    W_mu = dy.parameter(W_mu_p)\n",
    "    b_mu = dy.parameter(b_mu_p)\n",
    "\n",
    "    W_sig = dy.parameter(W_sig_p)\n",
    "    b_sig = dy.parameter(b_sig_p)\n",
    "    \n",
    "    # Slowly phase out getting both inputs\n",
    "    if random.random() < epsilon:\n",
    "        mask = dy.zeros(HIDDEN_DIM)\n",
    "    else:\n",
    "        mask = dy.ones(HIDDEN_DIM)\n",
    "        \n",
    "    if random.random() < 0.5:\n",
    "        mu_tweet = dy.cmult(mu_tweet, mask)\n",
    "        log_var_tweet = dy.cmult(log_var_tweet, mask)\n",
    "    else:\n",
    "        mu_tag = dy.cmult(mu_tag, mask)\n",
    "        log_var_tag = dy.cmult(log_var_tag, mask)\n",
    "    \n",
    "    mu      = dy.affine_transform([b_mu,  W_mu,  dy.concatenate([mu_tweet, mu_tag])])\n",
    "    log_var = dy.affine_transform([b_sig, W_sig, dy.concatenate([log_var_tweet, log_var_tag])])\n",
    "\n",
    "    # KL-Divergence loss computation\n",
    "    kl_loss = -0.5 * dy.sum_elems(1 + log_var - dy.pow(mu, dy.inputVector([2])) - dy.exp(log_var))\n",
    "\n",
    "    z = reparameterize(mu, log_var)\n",
    "\n",
    "    # now step through the output sentence\n",
    "    all_losses = []\n",
    "\n",
    "    current_state = lstm_decode.initial_state().set_s([z, dy.tanh(z)])\n",
    "    prev_word = src[0]\n",
    "    W_sm = dy.parameter(W_tweet_softmax_p)\n",
    "    b_sm = dy.parameter(b_tweet_softmax_p)\n",
    "\n",
    "    for next_word in src[1:]:\n",
    "        # feed the current state into the\n",
    "        \n",
    "        current_state = current_state.add_input(embed[prev_word])\n",
    "        output_embedding = current_state.output()\n",
    "\n",
    "        s = dy.affine_transform([b_sm, W_sm, output_embedding])\n",
    "        \n",
    "        all_losses.append(dy.pickneglogsoftmax(s, next_word))\n",
    "\n",
    "        # Slowly phase out teacher forcing (this may be slow??)\n",
    "        if random.random() < epsilon:\n",
    "            p = dy.softmax(s).npvalue()\n",
    "            prev_word = np.random.choice(VOCAB_SIZE, p=p/p.sum())\n",
    "        else:\n",
    "            prev_word = next_word\n",
    "    \n",
    "    softmax_loss = dy.esum(all_losses)\n",
    "\n",
    "    W_hidden = dy.parameter(W_hidden_p)\n",
    "    b_hidden = dy.parameter(b_hidden_p)\n",
    "    \n",
    "    W_out = dy.parameter(W_tag_output_p)\n",
    "    b_out = dy.parameter(b_tag_output_p)\n",
    "    \n",
    "    h = dy.dropout(dy.tanh(b_hidden + W_hidden * z), DROPOUT)\n",
    "    o = dy.logistic(b_out + W_out * h)\n",
    "    \n",
    "    crossentropy_loss = dy.binary_log_loss(o, tags_tensor)\n",
    "                               \n",
    "    return kl_loss, softmax_loss, crossentropy_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch size of 16.\n",
      "Training ... Iteration: 0 Epsilon: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91563ac34dcf41e4bc6af1142e791f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13304), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training\n",
    "print ('Using batch size of {}.'.format(BATCH_SIZE))\n",
    "\n",
    "epsilon = EPSILON_MIN\n",
    "kl_weight = KL_WEIGHT_START\n",
    "steps = 0\n",
    "strikes = 0\n",
    "last_dev_loss = np.inf\n",
    "for ITER in range(100):\n",
    "    # Perform training\n",
    "    random.shuffle(train)\n",
    "    \n",
    "    batches = [train[i:i + BATCH_SIZE] for i in range(0, len(train), BATCH_SIZE)]\n",
    "    \n",
    "    train_words, train_loss, train_kl_loss, train_reconstruct_loss, total_tag_loss = 0, 0.0, 0.0, 0.0, 0.0\n",
    "    start = time()\n",
    "    \n",
    "    print ('Training ... Iteration:', ITER, 'Epsilon:', epsilon)\n",
    "    for i, batch in enumerate(tqdm(batches)):\n",
    "        dy.renew_cg()\n",
    "#         dy.renew_cg(immediate_compute=True, check_validity=True) # makes kernel die\n",
    "        losses = []\n",
    "        for sent_id, sent in enumerate(batch):\n",
    "            if len(sent[1]) < 1 or len(sent[0]) < 3:\n",
    "                continue\n",
    "            kl_loss, softmax_loss, tag_loss = calc_loss(sent, epsilon)\n",
    "            #total_loss = dy.esum([kl_loss, softmax_loss, tag_loss])\n",
    "            #train_loss += total_loss.value()\n",
    "            \n",
    "            # Gradually increase KL-Divergence loss\n",
    "#             if steps < 15000:\n",
    "#                 kl_weight = 1 / (1 + np.exp(-0.001 * steps + 5))\n",
    "#             else:\n",
    "#                 kl_weight = 1.0\n",
    "\n",
    "            # Zero out KL weight\n",
    "            kl_weight = 0.0\n",
    "                \n",
    "            losses.append(dy.esum([kl_weight * kl_loss, softmax_loss, tag_loss]))\n",
    "\n",
    "            # Record the KL loss and reconstruction loss separately help you monitor the training.\n",
    "            train_kl_loss += kl_loss.value()\n",
    "            train_reconstruct_loss += softmax_loss.value()\n",
    "            total_tag_loss += tag_loss.value()\n",
    "            \n",
    "            train_words += len(sent[0])\n",
    "        steps += 1\n",
    "   \n",
    "        # Batch update\n",
    "        batch_loss = dy.esum(losses)/BATCH_SIZE\n",
    "        train_loss += batch_loss.value()\n",
    "        batch_loss.backward()\n",
    "        trainer.update()\n",
    "        \n",
    "        \n",
    "        #total_loss.backward()\n",
    "        #trainer.update()\n",
    "        #if (sent_id + 1) % 1000 == 0:\n",
    "        #    print(\"--finished %r sentences\" % (sent_id + 1))\n",
    "\n",
    "    # Gradually increase KL-Divergence loss\n",
    "    if steps < 100000:\n",
    "        epsilon = .9 / (1 + np.exp(-0.0001 * steps + 5))\n",
    "    else:\n",
    "        epsilon = EPSILON_MAX\n",
    "        \n",
    "    #epsilon = min(EPSILON_MAX, epsilon + 0.05)\n",
    "    print(\"iter %r: train loss/word=%.4f, kl loss/word=%.4f, reconstruction loss/word=%.4f, ppl=%.4f, tag loss=%.4fs\" % (\n",
    "        ITER, train_loss / train_words, train_kl_loss / train_words, train_reconstruct_loss / train_words,\n",
    "        math.exp(train_loss / train_words), total_tag_loss / len(train)))\n",
    "\n",
    "    # Evaluate on dev set\n",
    "    dev_words, dev_loss, dev_kl_loss, dev_reconstruct_loss, dev_tag_loss = 0, 0.0, 0.0, 0.0, 0.0\n",
    "    start = time()\n",
    "    print ('Evaluating batch ... ')\n",
    "    for sent_id, sent in enumerate(tqdm(dev)):\n",
    "        dy.renew_cg()\n",
    "        if len(sent[1]) < 1 or len(sent[0]) < 3:\n",
    "                continue\n",
    "        kl_loss, softmax_loss, tag_loss = calc_loss(sent)\n",
    "\n",
    "        dev_kl_loss += kl_loss.value()\n",
    "        dev_reconstruct_loss += softmax_loss.value()\n",
    "        dev_tag_loss += tag_loss.value()\n",
    "        dev_loss += kl_loss.value() + softmax_loss.value() + tag_loss.value()\n",
    "\n",
    "        dev_words += len(sent[0])\n",
    "        trainer.update()\n",
    "\n",
    "    print(\"iter %r: dev loss/word=%.4f, kl loss/word=%.4f, reconstruction loss/word=%.4f, ppl=%.4f, tag loss=%.2fs\" % (\n",
    "        ITER, dev_loss / dev_words, dev_kl_loss / dev_words, dev_reconstruct_loss / dev_words,\n",
    "        math.exp(dev_loss / dev_words), dev_tag_loss / len(dev)))\n",
    "    if dev_loss > last_dev_loss and ITER > 9:\n",
    "        strikes += 1\n",
    "    else:\n",
    "        strikes = 0\n",
    "        last_dev_loss = dev_loss\n",
    "        model.save('tweet_tag_vae.best.weights')\n",
    "        \n",
    "    if strikes >= PATIENCE:\n",
    "        print ('Early stopping after {} iterations.')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.save('trained_vae_joint_multimodal.weights.x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.populate('trained_vae_joint_multimodal.weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.populate('/usr2/mamille2/twitter/data/huang2016_data/tweet_tag_vae.best.weights')\n",
    "model.populate('/usr2/mamille2/twitter/data/huang2016_data/tweet_tag_vae.new.best.weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hallucinate_tags(tweet, sample=False, print_loss=False):\n",
    "    dy.renew_cg()\n",
    "    \n",
    "    # Transduce all batch elements with an LSTM\n",
    "    src = tweet\n",
    "\n",
    "    # initialize the LSTM\n",
    "    init_state_src = lstm_encode.initial_state()\n",
    "\n",
    "    # get the output of the first LSTM\n",
    "    src_output = init_state_src.add_inputs([embed[x] for x in src])[-1].output()\n",
    "\n",
    "    # Now compute mean and standard deviation of source hidden state.\n",
    "    W_mu_tweet = dy.parameter(W_mu_tweet_p)\n",
    "    V_mu_tweet = dy.parameter(V_mu_tweet_p)\n",
    "    b_mu_tweet = dy.parameter(b_mu_tweet_p)\n",
    "\n",
    "    W_sig_tweet = dy.parameter(W_sig_tweet_p)\n",
    "    V_sig_tweet = dy.parameter(V_sig_tweet_p)\n",
    "    b_sig_tweet = dy.parameter(b_sig_tweet_p)\n",
    "    \n",
    "    # Compute tweet encoding\n",
    "    mu_tweet      = mlp(src_output, W_mu_tweet,  V_mu_tweet,  b_mu_tweet)\n",
    "    log_var_tweet = mlp(src_output, W_sig_tweet, V_sig_tweet, b_sig_tweet)\n",
    "    \n",
    "    #W_mu_tag = dy.parameter(W_mu_tag_p)\n",
    "    #V_mu_tag = dy.parameter(V_mu_tag_p)\n",
    "    #b_mu_tag = dy.parameter(b_mu_tag_p)\n",
    "\n",
    "    #W_sig_tag = dy.parameter(W_sig_tag_p)\n",
    "    #V_sig_tag = dy.parameter(V_sig_tag_p)\n",
    "    #b_sig_tag = dy.parameter(b_sig_tag_p)\n",
    "    \n",
    "    # Compute tag encoding\n",
    "    #tags_tensor = dy.sparse_inputTensor([tags], np.ones((len(tags),)), (NUM_TAGS,))\n",
    "    \n",
    "    #mu_tag      = dy.dropout(mlp(tags_tensor, W_mu_tag,  V_mu_tag,  b_mu_tag), DROPOUT)\n",
    "    #log_var_tag = dy.dropout(mlp(tags_tensor, W_sig_tag, V_sig_tag, b_sig_tag), DROPOUT)\n",
    "    \n",
    "    # Combine encodings for mean and diagonal covariance\n",
    "    W_mu = dy.parameter(W_mu_p)\n",
    "    b_mu = dy.parameter(b_mu_p)\n",
    "\n",
    "    W_sig = dy.parameter(W_sig_p)\n",
    "    b_sig = dy.parameter(b_sig_p)\n",
    "    \n",
    "    \n",
    "    mu_tag = dy.zeros(HIDDEN_DIM)\n",
    "    log_var_tag = dy.zeros(HIDDEN_DIM)\n",
    "    \n",
    "    mu      = dy.affine_transform([b_mu,  W_mu,  dy.concatenate([mu_tweet, mu_tag])])\n",
    "    log_var = dy.affine_transform([b_sig, W_sig, dy.concatenate([log_var_tweet, log_var_tag])])\n",
    "\n",
    "    # KL-Divergence loss computation\n",
    "    kl_loss = -0.5 * dy.sum_elems(1 + log_var - dy.pow(mu, dy.inputVector([2])) - dy.exp(log_var))\n",
    "    \n",
    "    if print_loss:\n",
    "        print(\"kl loss/word={:.4f}\".format(kl_loss.value()/len(tweet)))\n",
    "\n",
    "    z = reparameterize(mu, log_var)\n",
    "\n",
    "    # now step through the output sentence\n",
    "#     all_losses = []\n",
    "\n",
    "    #current_state = lstm_decode.initial_state().set_s([z, dy.tanh(z)])\n",
    "    #prev_word = src[0]\n",
    "    #W_sm = dy.parameter(W_tweet_softmax_p)\n",
    "    #b_sm = dy.parameter(b_tweet_softmax_p)\n",
    "\n",
    "    #for next_word in src[1:]:\n",
    "    #    # feed the current state into the\n",
    "    #    current_state = current_state.add_input(embed[prev_word])\n",
    "    #    output_embedding = current_state.output()\n",
    "\n",
    "    #    s = dy.affine_transform([b_sm, W_sm, output_embedding])\n",
    "    #    all_losses.append(dy.pickneglogsoftmax(s, next_word))\n",
    "\n",
    "    #    prev_word = next_word\n",
    "    \n",
    "    #softmax_loss = dy.esum(all_losses)\n",
    "\n",
    "    W_hidden = dy.parameter(W_hidden_p)\n",
    "    b_hidden = dy.parameter(b_hidden_p)\n",
    "    \n",
    "    W_out = dy.parameter(W_tag_output_p)\n",
    "    b_out = dy.parameter(b_tag_output_p)\n",
    "    \n",
    "    h = dy.tanh(b_hidden + W_hidden * z)\n",
    "    o = dy.logistic(b_out + W_out * h)\n",
    "    \n",
    "    tag_ranks = o.value()\n",
    "    \n",
    "    # Sample from tags\n",
    "    if sample:\n",
    "        print('Sampling')\n",
    "        gen_tags = []\n",
    "        for i, p in enumerate(tag_ranks):\n",
    "            if random.random() < p:\n",
    "                gen_tags.append(i)\n",
    "\n",
    "        return gen_tags\n",
    "\n",
    "    else:\n",
    "        return tag_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(tweets, k=5):\n",
    "    \"\"\" \n",
    "        Returns precision, recall and f1 at a given k number of tags \n",
    "        \n",
    "        Args:\n",
    "            tweets: ([tweet wd indices], [gold tags indices])\n",
    "    \"\"\"\n",
    "    \n",
    "    prec = []\n",
    "    rec = []\n",
    "    any_matches = 0\n",
    "    \n",
    "    for i, (t, gold) in enumerate(tqdm(tweets)):\n",
    "        \n",
    "        if len(gold) == 0 or len(t) < 3: # should be excluded\n",
    "            continue\n",
    "        \n",
    "        # Get predicted tweets\n",
    "        if i % 1000 == 0:\n",
    "            pred = hallucinate_tags(t, print_loss=True)\n",
    "        \n",
    "        else:\n",
    "            pred = hallucinate_tags(t)\n",
    "            \n",
    "        top_pred_args = np.argsort(pred)[::-1][:k]\n",
    "        \n",
    "        # Compare with gold tweets\n",
    "        matches = sum(1 for t in top_pred_args if t in gold)\n",
    "        if matches > 0:\n",
    "            any_matches += matches\n",
    "            print(\"Total tag matches: {}\".format(any_matches))\n",
    "        prec.append(matches/k)\n",
    "        rec.append(matches/len(gold))\n",
    "        \n",
    "    # Compute averages, f1\n",
    "    avg_p = np.mean(prec)\n",
    "    avg_r = np.mean(rec)\n",
    "    mean_f1 = 2 * avg_p * avg_r / (avg_p + avg_r)\n",
    "\n",
    "    return avg_p, avg_r, mean_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "a = range(6)\n",
    "b = range(4,7)\n",
    "sum(1 for t in a if t in b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternative count\n",
    "len(set(a).intersection(set(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb4946310ed4efa8ef2284a29c2e48a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kl loss/word=0.0024\n",
      "Total tag matches: 1\n",
      "Total tag matches: 2\n",
      "Total tag matches: 3\n",
      "Total tag matches: 4\n",
      "Total tag matches: 5\n",
      "Total tag matches: 6\n",
      "Total tag matches: 7\n",
      "Total tag matches: 8\n",
      "Total tag matches: 9\n",
      "Total tag matches: 10\n",
      "Total tag matches: 11\n",
      "Total tag matches: 12\n",
      "Total tag matches: 13\n",
      "Total tag matches: 14\n",
      "Total tag matches: 15\n",
      "Total tag matches: 16\n",
      "Total tag matches: 17\n",
      "Total tag matches: 18\n",
      "Total tag matches: 19\n",
      "Total tag matches: 20\n",
      "Total tag matches: 21\n",
      "Total tag matches: 22\n",
      "Total tag matches: 23\n",
      "Total tag matches: 24\n",
      "Total tag matches: 25\n",
      "kl loss/word=0.0025\n",
      "Total tag matches: 26\n",
      "Total tag matches: 27\n",
      "Total tag matches: 28\n",
      "Total tag matches: 29\n",
      "Total tag matches: 30\n",
      "Total tag matches: 31\n",
      "Total tag matches: 32\n",
      "Total tag matches: 33\n",
      "Total tag matches: 34\n",
      "kl loss/word=0.0047\n",
      "Total tag matches: 35\n",
      "Total tag matches: 36\n",
      "Total tag matches: 37\n",
      "Total tag matches: 38\n",
      "Total tag matches: 39\n",
      "Total tag matches: 40\n",
      "Total tag matches: 41\n",
      "kl loss/word=0.0025\n",
      "Total tag matches: 42\n",
      "Total tag matches: 43\n",
      "Total tag matches: 44\n",
      "Total tag matches: 45\n",
      "Total tag matches: 46\n",
      "Total tag matches: 47\n",
      "Total tag matches: 48\n",
      "Total tag matches: 49\n",
      "Total tag matches: 50\n",
      "Total tag matches: 51\n",
      "Total tag matches: 52\n",
      "Total tag matches: 53\n",
      "Total tag matches: 54\n",
      "Total tag matches: 55\n",
      "kl loss/word=0.0021\n",
      "Total tag matches: 56\n",
      "Total tag matches: 57\n",
      "Total tag matches: 58\n",
      "Total tag matches: 59\n",
      "Total tag matches: 60\n",
      "Total tag matches: 61\n",
      "Total tag matches: 62\n",
      "kl loss/word=0.0033\n",
      "Total tag matches: 63\n",
      "Total tag matches: 64\n",
      "Total tag matches: 65\n",
      "Total tag matches: 66\n",
      "Total tag matches: 67\n",
      "Total tag matches: 68\n",
      "Total tag matches: 69\n",
      "Total tag matches: 70\n",
      "Total tag matches: 71\n",
      "kl loss/word=0.0027\n",
      "Total tag matches: 72\n",
      "Total tag matches: 73\n",
      "Total tag matches: 74\n",
      "Total tag matches: 75\n",
      "Total tag matches: 76\n",
      "Total tag matches: 77\n",
      "Total tag matches: 78\n",
      "Total tag matches: 79\n",
      "Total tag matches: 80\n",
      "Total tag matches: 81\n",
      "Total tag matches: 82\n",
      "Total tag matches: 83\n",
      "kl loss/word=0.0039\n",
      "Total tag matches: 84\n",
      "Total tag matches: 85\n",
      "Total tag matches: 86\n",
      "Total tag matches: 87\n",
      "Total tag matches: 88\n",
      "Total tag matches: 89\n",
      "Total tag matches: 90\n",
      "Total tag matches: 91\n",
      "Total tag matches: 92\n",
      "Total tag matches: 93\n",
      "kl loss/word=0.0035\n",
      "Total tag matches: 94\n",
      "Total tag matches: 95\n",
      "Total tag matches: 96\n",
      "Total tag matches: 97\n",
      "Total tag matches: 98\n",
      "Total tag matches: 99\n",
      "Total tag matches: 100\n",
      "Total tag matches: 101\n",
      "Total tag matches: 102\n",
      "Total tag matches: 103\n",
      "Total tag matches: 105\n",
      "kl loss/word=0.0085\n",
      "Total tag matches: 106\n",
      "Total tag matches: 107\n",
      "Total tag matches: 108\n",
      "Total tag matches: 109\n",
      "Total tag matches: 110\n",
      "Total tag matches: 111\n",
      "Total tag matches: 112\n",
      "kl loss/word=0.0021\n",
      "Total tag matches: 113\n",
      "Total tag matches: 114\n",
      "Total tag matches: 115\n",
      "Total tag matches: 116\n",
      "Total tag matches: 117\n",
      "Total tag matches: 118\n",
      "Total tag matches: 119\n",
      "Total tag matches: 120\n",
      "Total tag matches: 121\n",
      "Total tag matches: 122\n",
      "Total tag matches: 123\n",
      "Total tag matches: 124\n",
      "Total tag matches: 125\n",
      "Total tag matches: 126\n",
      "Total tag matches: 127\n",
      "kl loss/word=0.0035\n",
      "Total tag matches: 128\n",
      "Total tag matches: 129\n",
      "Total tag matches: 130\n",
      "Total tag matches: 131\n",
      "Total tag matches: 132\n",
      "Total tag matches: 133\n",
      "Total tag matches: 134\n",
      "Total tag matches: 135\n",
      "Total tag matches: 136\n",
      "Total tag matches: 137\n",
      "Total tag matches: 138\n",
      "Total tag matches: 139\n",
      "kl loss/word=0.0043\n",
      "Total tag matches: 140\n",
      "Total tag matches: 141\n",
      "Total tag matches: 142\n",
      "Total tag matches: 143\n",
      "Total tag matches: 144\n",
      "Total tag matches: 145\n",
      "kl loss/word=0.0033\n",
      "Total tag matches: 146\n",
      "Total tag matches: 147\n",
      "Total tag matches: 148\n",
      "Total tag matches: 149\n",
      "Total tag matches: 150\n",
      "Total tag matches: 151\n",
      "Total tag matches: 152\n",
      "kl loss/word=0.0047\n",
      "Total tag matches: 153\n",
      "Total tag matches: 154\n",
      "Total tag matches: 156\n",
      "Total tag matches: 157\n",
      "Total tag matches: 158\n",
      "Total tag matches: 159\n",
      "Total tag matches: 160\n",
      "Total tag matches: 161\n",
      "Total tag matches: 162\n",
      "Total tag matches: 163\n",
      "Total tag matches: 165\n",
      "Total tag matches: 166\n",
      "kl loss/word=0.0024\n",
      "Total tag matches: 167\n",
      "Total tag matches: 168\n",
      "Total tag matches: 169\n",
      "Total tag matches: 170\n",
      "Total tag matches: 171\n",
      "Total tag matches: 172\n",
      "Total tag matches: 173\n",
      "Total tag matches: 174\n",
      "Total tag matches: 175\n",
      "Total tag matches: 176\n",
      "Total tag matches: 177\n",
      "kl loss/word=0.0027\n",
      "Total tag matches: 178\n",
      "Total tag matches: 179\n",
      "Total tag matches: 180\n",
      "Total tag matches: 181\n",
      "Total tag matches: 182\n",
      "Total tag matches: 183\n",
      "Total tag matches: 184\n",
      "Total tag matches: 185\n",
      "Total tag matches: 186\n",
      "Total tag matches: 187\n",
      "Total tag matches: 188\n",
      "Total tag matches: 189\n",
      "Total tag matches: 190\n",
      "Total tag matches: 191\n",
      "Total tag matches: 192\n",
      "Total tag matches: 193\n",
      "Total tag matches: 194\n",
      "Total tag matches: 195\n",
      "kl loss/word=0.0021\n",
      "Total tag matches: 196\n",
      "Total tag matches: 197\n",
      "Total tag matches: 198\n",
      "Total tag matches: 199\n",
      "Total tag matches: 200\n",
      "Total tag matches: 201\n",
      "Total tag matches: 202\n",
      "Total tag matches: 203\n",
      "Total tag matches: 204\n",
      "Total tag matches: 205\n",
      "Total tag matches: 206\n",
      "Total tag matches: 207\n",
      "Total tag matches: 208\n",
      "kl loss/word=0.0025\n",
      "Total tag matches: 209\n",
      "Total tag matches: 210\n",
      "Total tag matches: 211\n",
      "Total tag matches: 212\n",
      "Total tag matches: 213\n",
      "\n",
      "k=5\tprecision: 0.0024212799818119817\trecall: 0.009577128566556781\tf1: 0.0038653309041716617\n"
     ]
    }
   ],
   "source": [
    "# Get evaluations\n",
    "\n",
    "eval_k = {} # {k: (prec, recall, f1)}\n",
    "\n",
    "# for k in range(1,6):\n",
    "for k in range(5,6):\n",
    "    eval_k[k] = evaluate(test, k)\n",
    "    print(\"k={}\\tprecision: {}\\trecall: {}\\tf1: {}\".format(k, eval_k[k][0], eval_k[k][1], eval_k[k][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5: (0.0024212799818119817, 0.0095771285665567814, 0.0038653309041716617)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc409c0a17d4f2989cc1e8fa4e537a4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kl loss/word=0.0020\n",
      "Total tag matches: 1\n",
      "kl loss/word=0.0016\n",
      "Total tag matches: 2\n",
      "Total tag matches: 3\n",
      "Total tag matches: 4\n",
      "kl loss/word=0.0013\n",
      "Total tag matches: 5\n",
      "Total tag matches: 6\n",
      "kl loss/word=0.0022\n",
      "Total tag matches: 7\n",
      "Total tag matches: 8\n",
      "Total tag matches: 9\n",
      "Total tag matches: 10\n",
      "kl loss/word=0.0019\n",
      "Total tag matches: 11\n",
      "Total tag matches: 12\n",
      "Total tag matches: 13\n",
      "kl loss/word=0.0014\n",
      "Total tag matches: 14\n",
      "Total tag matches: 15\n",
      "Total tag matches: 16\n",
      "Total tag matches: 17\n",
      "Total tag matches: 18\n",
      "kl loss/word=0.0024\n",
      "Total tag matches: 19\n",
      "kl loss/word=0.0014\n",
      "Total tag matches: 20\n",
      "Total tag matches: 21\n",
      "Total tag matches: 22\n",
      "Total tag matches: 23\n",
      "Total tag matches: 24\n",
      "Total tag matches: 25\n",
      "Total tag matches: 26\n",
      "kl loss/word=0.0019\n",
      "Total tag matches: 27\n",
      "Total tag matches: 28\n",
      "kl loss/word=0.0035\n",
      "Total tag matches: 29\n",
      "Total tag matches: 30\n",
      "Total tag matches: 31\n",
      "Total tag matches: 32\n",
      "Total tag matches: 33\n",
      "Total tag matches: 34\n",
      "kl loss/word=0.0019\n",
      "Total tag matches: 35\n",
      "kl loss/word=0.0021\n",
      "Total tag matches: 36\n",
      "Total tag matches: 37\n",
      "Total tag matches: 38\n",
      "Total tag matches: 39\n",
      "kl loss/word=0.0025\n",
      "Total tag matches: 40\n",
      "Total tag matches: 41\n",
      "Total tag matches: 42\n",
      "Total tag matches: 43\n",
      "kl loss/word=0.0020\n",
      "Total tag matches: 44\n",
      "Total tag matches: 45\n",
      "kl loss/word=0.0030\n",
      "Total tag matches: 46\n",
      "Total tag matches: 47\n",
      "kl loss/word=0.0022\n",
      "Total tag matches: 48\n",
      "Total tag matches: 49\n",
      "Total tag matches: 50\n",
      "kl loss/word=0.0024\n",
      "Total tag matches: 51\n",
      "Total tag matches: 52\n",
      "Total tag matches: 53\n",
      "kl loss/word=0.0021\n",
      "Total tag matches: 54\n",
      "kl loss/word=0.0027\n",
      "Total tag matches: 55\n",
      "Total tag matches: 56\n",
      "Total tag matches: 57\n",
      "Total tag matches: 58\n",
      "kl loss/word=0.0033\n",
      "kl loss/word=0.0019\n",
      "Total tag matches: 59\n",
      "Total tag matches: 60\n",
      "kl loss/word=0.0030\n",
      "Total tag matches: 61\n",
      "Total tag matches: 62\n",
      "kl loss/word=0.0015\n",
      "Total tag matches: 63\n",
      "\n",
      "k=1\tprecision: 0.002806361085126286\trecall: 0.0021381798743819323\tf1: 0.002427123100649761\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf7ceb609cd4f9d9863067a6db7d37f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kl loss/word=0.0020\n",
      "Total tag matches: 1\n",
      "kl loss/word=0.0016\n",
      "Total tag matches: 2\n",
      "Total tag matches: 3\n",
      "Total tag matches: 4\n",
      "Total tag matches: 5\n",
      "Total tag matches: 6\n",
      "kl loss/word=0.0013\n",
      "Total tag matches: 7\n",
      "kl loss/word=0.0022\n",
      "Total tag matches: 8\n",
      "Total tag matches: 9\n",
      "Total tag matches: 10\n",
      "kl loss/word=0.0019\n",
      "Total tag matches: 11\n",
      "Total tag matches: 12\n",
      "Total tag matches: 13\n",
      "Total tag matches: 14\n",
      "kl loss/word=0.0014\n",
      "Total tag matches: 15\n",
      "Total tag matches: 16\n",
      "Total tag matches: 17\n",
      "Total tag matches: 18\n",
      "Total tag matches: 19\n",
      "Total tag matches: 20\n",
      "Total tag matches: 21\n",
      "Total tag matches: 22\n",
      "Total tag matches: 23\n",
      "kl loss/word=0.0024\n",
      "Total tag matches: 24\n",
      "Total tag matches: 25\n",
      "kl loss/word=0.0014\n",
      "Total tag matches: 26\n",
      "Total tag matches: 27\n",
      "Total tag matches: 28\n",
      "Total tag matches: 29\n",
      "Total tag matches: 30\n",
      "kl loss/word=0.0019\n",
      "Total tag matches: 31\n",
      "Total tag matches: 32\n",
      "Total tag matches: 33\n",
      "Total tag matches: 34\n",
      "kl loss/word=0.0035\n",
      "Total tag matches: 35\n",
      "Total tag matches: 36\n",
      "Total tag matches: 37\n",
      "Total tag matches: 38\n",
      "Total tag matches: 39\n",
      "kl loss/word=0.0019\n",
      "Total tag matches: 40\n",
      "Total tag matches: 41\n",
      "kl loss/word=0.0021\n",
      "Total tag matches: 42\n",
      "Total tag matches: 43\n",
      "Total tag matches: 44\n",
      "Total tag matches: 45\n",
      "kl loss/word=0.0025\n",
      "Total tag matches: 46\n",
      "Total tag matches: 47\n",
      "Total tag matches: 48\n",
      "Total tag matches: 49\n",
      "kl loss/word=0.0020\n",
      "Total tag matches: 50\n",
      "Total tag matches: 51\n",
      "Total tag matches: 52\n",
      "Total tag matches: 53\n",
      "kl loss/word=0.0030\n",
      "Total tag matches: 54\n",
      "Total tag matches: 55\n",
      "Total tag matches: 56\n",
      "Total tag matches: 57\n",
      "kl loss/word=0.0022\n",
      "Total tag matches: 58\n",
      "Total tag matches: 59\n",
      "Total tag matches: 60\n",
      "Total tag matches: 61\n",
      "kl loss/word=0.0024\n",
      "Total tag matches: 62\n",
      "Total tag matches: 63\n",
      "Total tag matches: 64\n",
      "Total tag matches: 65\n",
      "kl loss/word=0.0021\n",
      "Total tag matches: 66\n",
      "Total tag matches: 67\n",
      "Total tag matches: 68\n",
      "Total tag matches: 69\n",
      "Total tag matches: 70\n",
      "Total tag matches: 71\n",
      "Total tag matches: 72\n",
      "Total tag matches: 73\n",
      "kl loss/word=0.0027\n",
      "Total tag matches: 74\n",
      "Total tag matches: 75\n",
      "Total tag matches: 76\n",
      "Total tag matches: 77\n",
      "Total tag matches: 78\n",
      "Total tag matches: 79\n",
      "Total tag matches: 80\n",
      "Total tag matches: 81\n",
      "Total tag matches: 82\n",
      "kl loss/word=0.0033\n",
      "Total tag matches: 83\n",
      "Total tag matches: 84\n",
      "Total tag matches: 85\n",
      "Total tag matches: 86\n",
      "Total tag matches: 87\n",
      "Total tag matches: 88\n",
      "kl loss/word=0.0019\n",
      "Total tag matches: 89\n",
      "Total tag matches: 90\n",
      "Total tag matches: 91\n",
      "Total tag matches: 92\n",
      "Total tag matches: 93\n",
      "Total tag matches: 94\n",
      "Total tag matches: 95\n",
      "Total tag matches: 96\n",
      "kl loss/word=0.0030\n",
      "Total tag matches: 97\n",
      "Total tag matches: 98\n",
      "Total tag matches: 99\n",
      "Total tag matches: 100\n",
      "Total tag matches: 101\n",
      "Total tag matches: 102\n",
      "Total tag matches: 103\n",
      "Total tag matches: 104\n",
      "Total tag matches: 105\n",
      "Total tag matches: 106\n",
      "kl loss/word=0.0015\n",
      "Total tag matches: 107\n",
      "Total tag matches: 108\n",
      "Total tag matches: 109\n",
      "Total tag matches: 110\n",
      "\n",
      "k=2\tprecision: 0.0024499977727292974\trecall: 0.003697269366118758\tf1: 0.0029470987700946616\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9cbae08e2247f3a68e0e099156cf5f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kl loss/word=0.0020\n",
      "Total tag matches: 1\n",
      "kl loss/word=0.0016\n",
      "Total tag matches: 2\n",
      "Total tag matches: 3\n",
      "Total tag matches: 4\n",
      "Total tag matches: 5\n",
      "Total tag matches: 6\n",
      "Total tag matches: 7\n",
      "Total tag matches: 8\n",
      "Total tag matches: 9\n",
      "Total tag matches: 10\n",
      "kl loss/word=0.0013\n",
      "Total tag matches: 11\n",
      "Total tag matches: 12\n",
      "Total tag matches: 13\n",
      "Total tag matches: 14\n",
      "Total tag matches: 15\n",
      "Total tag matches: 16\n",
      "Total tag matches: 17\n",
      "kl loss/word=0.0022\n",
      "Total tag matches: 18\n",
      "Total tag matches: 19\n",
      "Total tag matches: 20\n",
      "Total tag matches: 21\n",
      "Total tag matches: 22\n",
      "Total tag matches: 23\n",
      "kl loss/word=0.0019\n",
      "Total tag matches: 24\n",
      "Total tag matches: 25\n",
      "Total tag matches: 26\n",
      "kl loss/word=0.0014\n",
      "Total tag matches: 27\n",
      "Total tag matches: 28\n",
      "Total tag matches: 29\n",
      "Total tag matches: 30\n",
      "Total tag matches: 31\n",
      "Total tag matches: 32\n",
      "Total tag matches: 33\n",
      "Total tag matches: 34\n",
      "Total tag matches: 35\n",
      "Total tag matches: 36\n",
      "Total tag matches: 37\n",
      "Total tag matches: 38\n",
      "Total tag matches: 39\n",
      "Total tag matches: 40\n",
      "kl loss/word=0.0024\n",
      "Total tag matches: 41\n",
      "Total tag matches: 42\n",
      "kl loss/word=0.0014\n",
      "Total tag matches: 43\n",
      "Total tag matches: 44\n",
      "Total tag matches: 45\n",
      "Total tag matches: 46\n",
      "Total tag matches: 47\n",
      "Total tag matches: 48\n",
      "Total tag matches: 49\n",
      "Total tag matches: 50\n",
      "kl loss/word=0.0019\n",
      "Total tag matches: 51\n",
      "Total tag matches: 52\n",
      "Total tag matches: 53\n",
      "Total tag matches: 54\n",
      "kl loss/word=0.0035\n",
      "Total tag matches: 55\n",
      "Total tag matches: 56\n",
      "Total tag matches: 57\n",
      "Total tag matches: 58\n",
      "Total tag matches: 59\n",
      "Total tag matches: 60\n",
      "Total tag matches: 61\n",
      "Total tag matches: 62\n",
      "kl loss/word=0.0019\n",
      "Total tag matches: 63\n",
      "Total tag matches: 64\n",
      "Total tag matches: 65\n",
      "Total tag matches: 66\n",
      "kl loss/word=0.0021\n",
      "Total tag matches: 68\n",
      "Total tag matches: 69\n",
      "Total tag matches: 70\n",
      "Total tag matches: 71\n",
      "Total tag matches: 72\n",
      "Total tag matches: 73\n",
      "Total tag matches: 74\n",
      "Total tag matches: 75\n",
      "Total tag matches: 76\n",
      "Total tag matches: 77\n",
      "Total tag matches: 78\n",
      "kl loss/word=0.0025\n",
      "Total tag matches: 79\n",
      "Total tag matches: 80\n",
      "Total tag matches: 81\n",
      "Total tag matches: 82\n",
      "Total tag matches: 83\n",
      "Total tag matches: 84\n",
      "Total tag matches: 85\n",
      "kl loss/word=0.0020\n",
      "Total tag matches: 86\n",
      "Total tag matches: 87\n",
      "Total tag matches: 88\n",
      "Total tag matches: 89\n",
      "Total tag matches: 90\n",
      "Total tag matches: 91\n",
      "Total tag matches: 92\n",
      "Total tag matches: 93\n",
      "kl loss/word=0.0030\n",
      "Total tag matches: 94\n",
      "Total tag matches: 95\n",
      "Total tag matches: 96\n",
      "Total tag matches: 97\n",
      "Total tag matches: 98\n",
      "Total tag matches: 99\n",
      "Total tag matches: 100\n",
      "kl loss/word=0.0022\n",
      "Total tag matches: 101\n",
      "Total tag matches: 102\n",
      "Total tag matches: 103\n",
      "kl loss/word=0.0024\n",
      "Total tag matches: 104\n",
      "Total tag matches: 105\n",
      "Total tag matches: 106\n",
      "Total tag matches: 107\n",
      "Total tag matches: 108\n",
      "Total tag matches: 109\n",
      "Total tag matches: 110\n",
      "Total tag matches: 111\n",
      "kl loss/word=0.0021\n",
      "Total tag matches: 112\n",
      "Total tag matches: 113\n",
      "Total tag matches: 114\n",
      "Total tag matches: 115\n",
      "Total tag matches: 116\n",
      "Total tag matches: 117\n",
      "Total tag matches: 118\n",
      "Total tag matches: 119\n",
      "kl loss/word=0.0027\n",
      "Total tag matches: 120\n",
      "Total tag matches: 121\n",
      "Total tag matches: 122\n",
      "Total tag matches: 123\n",
      "Total tag matches: 124\n",
      "Total tag matches: 125\n",
      "Total tag matches: 126\n",
      "Total tag matches: 127\n",
      "Total tag matches: 128\n",
      "Total tag matches: 129\n",
      "Total tag matches: 130\n",
      "Total tag matches: 131\n",
      "kl loss/word=0.0033\n",
      "Total tag matches: 132\n",
      "Total tag matches: 133\n",
      "Total tag matches: 134\n",
      "Total tag matches: 135\n",
      "Total tag matches: 136\n",
      "Total tag matches: 137\n",
      "Total tag matches: 138\n",
      "Total tag matches: 139\n",
      "Total tag matches: 140\n",
      "Total tag matches: 141\n",
      "Total tag matches: 142\n",
      "Total tag matches: 143\n",
      "kl loss/word=0.0019\n",
      "Total tag matches: 144\n",
      "Total tag matches: 145\n",
      "Total tag matches: 146\n",
      "Total tag matches: 147\n",
      "Total tag matches: 148\n",
      "Total tag matches: 149\n",
      "kl loss/word=0.0030\n",
      "Total tag matches: 150\n",
      "Total tag matches: 151\n",
      "Total tag matches: 152\n",
      "Total tag matches: 153\n",
      "Total tag matches: 154\n",
      "Total tag matches: 155\n",
      "Total tag matches: 156\n",
      "Total tag matches: 157\n",
      "Total tag matches: 158\n",
      "Total tag matches: 159\n",
      "kl loss/word=0.0015\n",
      "Total tag matches: 160\n",
      "\n",
      "k=3\tprecision: 0.0023757554159799248\trecall: 0.00521181344380596\tf1: 0.003263757929584946\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5336ffaaa0444b928803d4fc22cbc021"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kl loss/word=0.0020\n",
      "Total tag matches: 1\n",
      "Total tag matches: 2\n",
      "Total tag matches: 3\n",
      "Total tag matches: 4\n",
      "Total tag matches: 5\n",
      "Total tag matches: 6\n",
      "kl loss/word=0.0016\n",
      "Total tag matches: 7\n",
      "Total tag matches: 8\n",
      "Total tag matches: 9\n",
      "Total tag matches: 10\n",
      "Total tag matches: 11\n",
      "Total tag matches: 12\n",
      "Total tag matches: 13\n",
      "Total tag matches: 14\n",
      "Total tag matches: 15\n",
      "Total tag matches: 16\n",
      "Total tag matches: 17\n",
      "Total tag matches: 18\n",
      "Total tag matches: 19\n",
      "kl loss/word=0.0013\n",
      "Total tag matches: 20\n",
      "Total tag matches: 21\n",
      "Total tag matches: 22\n",
      "Total tag matches: 23\n",
      "Total tag matches: 24\n",
      "Total tag matches: 25\n",
      "Total tag matches: 26\n",
      "kl loss/word=0.0022\n",
      "Total tag matches: 27\n",
      "Total tag matches: 28\n",
      "Total tag matches: 29\n",
      "Total tag matches: 30\n",
      "Total tag matches: 31\n",
      "kl loss/word=0.0019\n",
      "Total tag matches: 32\n",
      "Total tag matches: 33\n",
      "Total tag matches: 34\n",
      "Total tag matches: 35\n",
      "Total tag matches: 36\n",
      "Total tag matches: 37\n",
      "Total tag matches: 38\n",
      "Total tag matches: 39\n",
      "Total tag matches: 40\n",
      "Total tag matches: 41\n",
      "Total tag matches: 42\n",
      "kl loss/word=0.0014\n",
      "Total tag matches: 43\n",
      "Total tag matches: 44\n",
      "Total tag matches: 45\n",
      "Total tag matches: 46\n",
      "Total tag matches: 47\n",
      "Total tag matches: 48\n",
      "Total tag matches: 49\n",
      "Total tag matches: 50\n",
      "Total tag matches: 51\n",
      "Total tag matches: 52\n",
      "Total tag matches: 53\n",
      "Total tag matches: 54\n",
      "Total tag matches: 55\n",
      "Total tag matches: 56\n",
      "kl loss/word=0.0024\n",
      "Total tag matches: 57\n",
      "Total tag matches: 58\n",
      "Total tag matches: 59\n",
      "Total tag matches: 60\n",
      "Total tag matches: 61\n",
      "kl loss/word=0.0014\n",
      "Total tag matches: 62\n",
      "Total tag matches: 63\n",
      "Total tag matches: 64\n",
      "Total tag matches: 65\n",
      "Total tag matches: 66\n",
      "Total tag matches: 67\n",
      "Total tag matches: 68\n",
      "Total tag matches: 69\n",
      "Total tag matches: 70\n",
      "Total tag matches: 71\n",
      "Total tag matches: 72\n",
      "Total tag matches: 73\n",
      "kl loss/word=0.0019\n",
      "Total tag matches: 74\n",
      "Total tag matches: 75\n",
      "Total tag matches: 76\n",
      "Total tag matches: 77\n",
      "Total tag matches: 78\n",
      "Total tag matches: 79\n",
      "Total tag matches: 80\n",
      "kl loss/word=0.0035\n",
      "Total tag matches: 81\n",
      "Total tag matches: 82\n",
      "Total tag matches: 83\n",
      "Total tag matches: 84\n",
      "Total tag matches: 85\n",
      "Total tag matches: 86\n",
      "Total tag matches: 87\n",
      "Total tag matches: 88\n",
      "kl loss/word=0.0019\n",
      "Total tag matches: 89\n",
      "Total tag matches: 90\n",
      "Total tag matches: 91\n",
      "Total tag matches: 92\n",
      "Total tag matches: 93\n",
      "Total tag matches: 94\n",
      "Total tag matches: 95\n",
      "Total tag matches: 96\n",
      "Total tag matches: 97\n",
      "Total tag matches: 98\n",
      "Total tag matches: 99\n",
      "Total tag matches: 100\n",
      "Total tag matches: 101\n",
      "Total tag matches: 102\n",
      "Total tag matches: 103\n",
      "Total tag matches: 104\n",
      "Total tag matches: 105\n",
      "Total tag matches: 106\n",
      "kl loss/word=0.0021\n",
      "Total tag matches: 107\n",
      "Total tag matches: 108\n",
      "Total tag matches: 109\n",
      "Total tag matches: 110\n",
      "Total tag matches: 111\n",
      "Total tag matches: 112\n",
      "Total tag matches: 113\n",
      "Total tag matches: 114\n",
      "Total tag matches: 115\n",
      "Total tag matches: 116\n",
      "Total tag matches: 117\n",
      "Total tag matches: 118\n",
      "Total tag matches: 119\n",
      "Total tag matches: 120\n",
      "Total tag matches: 121\n",
      "Total tag matches: 122\n",
      "Total tag matches: 123\n",
      "Total tag matches: 124\n",
      "Total tag matches: 125\n",
      "kl loss/word=0.0025\n",
      "Total tag matches: 126\n",
      "Total tag matches: 127\n",
      "Total tag matches: 128\n",
      "Total tag matches: 129\n",
      "Total tag matches: 130\n",
      "Total tag matches: 131\n",
      "Total tag matches: 132\n",
      "Total tag matches: 133\n",
      "Total tag matches: 134\n",
      "Total tag matches: 135\n",
      "Total tag matches: 136\n",
      "kl loss/word=0.0020\n",
      "Total tag matches: 137\n",
      "Total tag matches: 138\n",
      "Total tag matches: 139\n",
      "Total tag matches: 140\n",
      "Total tag matches: 141\n",
      "Total tag matches: 142\n",
      "Total tag matches: 143\n",
      "kl loss/word=0.0030\n",
      "Total tag matches: 144\n",
      "Total tag matches: 145\n",
      "Total tag matches: 146\n",
      "Total tag matches: 147\n",
      "Total tag matches: 148\n",
      "Total tag matches: 149\n",
      "Total tag matches: 150\n",
      "Total tag matches: 151\n",
      "kl loss/word=0.0022\n",
      "Total tag matches: 152\n",
      "Total tag matches: 153\n",
      "Total tag matches: 154\n",
      "Total tag matches: 155\n",
      "Total tag matches: 156\n",
      "Total tag matches: 157\n",
      "Total tag matches: 158\n",
      "kl loss/word=0.0024\n",
      "Total tag matches: 159\n",
      "Total tag matches: 160\n",
      "Total tag matches: 161\n",
      "Total tag matches: 162\n",
      "Total tag matches: 163\n",
      "Total tag matches: 164\n",
      "kl loss/word=0.0021\n",
      "Total tag matches: 165\n",
      "Total tag matches: 166\n",
      "Total tag matches: 167\n",
      "Total tag matches: 168\n",
      "Total tag matches: 169\n",
      "Total tag matches: 170\n",
      "Total tag matches: 171\n",
      "kl loss/word=0.0027\n",
      "Total tag matches: 172\n",
      "Total tag matches: 173\n",
      "Total tag matches: 174\n",
      "Total tag matches: 175\n",
      "Total tag matches: 176\n",
      "Total tag matches: 177\n",
      "Total tag matches: 178\n",
      "Total tag matches: 179\n",
      "Total tag matches: 180\n",
      "Total tag matches: 181\n",
      "Total tag matches: 182\n",
      "Total tag matches: 183\n",
      "Total tag matches: 184\n",
      "Total tag matches: 185\n",
      "Total tag matches: 186\n",
      "Total tag matches: 187\n",
      "Total tag matches: 188\n",
      "Total tag matches: 189\n",
      "Total tag matches: 190\n",
      "kl loss/word=0.0033\n",
      "Total tag matches: 191\n",
      "Total tag matches: 192\n",
      "Total tag matches: 193\n",
      "Total tag matches: 194\n",
      "Total tag matches: 195\n",
      "Total tag matches: 196\n",
      "Total tag matches: 197\n",
      "Total tag matches: 198\n",
      "Total tag matches: 199\n",
      "Total tag matches: 200\n",
      "kl loss/word=0.0019\n",
      "Total tag matches: 201\n",
      "Total tag matches: 202\n",
      "Total tag matches: 203\n",
      "Total tag matches: 204\n",
      "Total tag matches: 205\n",
      "Total tag matches: 206\n",
      "Total tag matches: 207\n",
      "Total tag matches: 208\n",
      "kl loss/word=0.0030\n",
      "Total tag matches: 209\n",
      "Total tag matches: 210\n",
      "Total tag matches: 211\n",
      "Total tag matches: 212\n",
      "Total tag matches: 213\n",
      "Total tag matches: 214\n",
      "Total tag matches: 215\n",
      "Total tag matches: 216\n",
      "Total tag matches: 217\n",
      "Total tag matches: 218\n",
      "Total tag matches: 219\n",
      "Total tag matches: 220\n",
      "Total tag matches: 221\n",
      "Total tag matches: 222\n",
      "Total tag matches: 223\n",
      "kl loss/word=0.0015\n",
      "Total tag matches: 224\n",
      "Total tag matches: 225\n",
      "Total tag matches: 226\n",
      "Total tag matches: 227\n",
      "\n",
      "k=4\tprecision: 0.002527952247316139\trecall: 0.007572720388436011\tf1: 0.0037905347920065583\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e23bfd8fce4286a3a0da2565e9e48f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kl loss/word=0.0020\n",
      "Total tag matches: 1\n",
      "Total tag matches: 2\n",
      "Total tag matches: 3\n",
      "Total tag matches: 4\n",
      "Total tag matches: 5\n",
      "Total tag matches: 6\n",
      "Total tag matches: 7\n",
      "Total tag matches: 8\n",
      "Total tag matches: 9\n",
      "kl loss/word=0.0016\n",
      "Total tag matches: 10\n",
      "Total tag matches: 11\n",
      "Total tag matches: 12\n",
      "Total tag matches: 13\n",
      "Total tag matches: 14\n",
      "Total tag matches: 15\n",
      "Total tag matches: 16\n",
      "Total tag matches: 17\n",
      "Total tag matches: 18\n",
      "Total tag matches: 19\n",
      "Total tag matches: 20\n",
      "Total tag matches: 21\n",
      "Total tag matches: 22\n",
      "Total tag matches: 23\n",
      "Total tag matches: 24\n",
      "kl loss/word=0.0013\n",
      "Total tag matches: 25\n",
      "Total tag matches: 26\n",
      "Total tag matches: 27\n",
      "Total tag matches: 28\n",
      "Total tag matches: 29\n",
      "Total tag matches: 30\n",
      "Total tag matches: 31\n",
      "Total tag matches: 32\n",
      "Total tag matches: 33\n",
      "kl loss/word=0.0022\n",
      "Total tag matches: 34\n",
      "Total tag matches: 35\n",
      "Total tag matches: 36\n",
      "Total tag matches: 37\n",
      "Total tag matches: 38\n",
      "Total tag matches: 39\n",
      "Total tag matches: 40\n",
      "kl loss/word=0.0019\n",
      "Total tag matches: 41\n",
      "Total tag matches: 42\n",
      "Total tag matches: 43\n",
      "Total tag matches: 44\n",
      "Total tag matches: 45\n",
      "Total tag matches: 46\n",
      "Total tag matches: 47\n",
      "Total tag matches: 48\n",
      "Total tag matches: 49\n",
      "Total tag matches: 50\n",
      "Total tag matches: 51\n",
      "kl loss/word=0.0014\n",
      "Total tag matches: 52\n",
      "Total tag matches: 53\n",
      "Total tag matches: 55\n",
      "Total tag matches: 56\n",
      "Total tag matches: 57\n",
      "Total tag matches: 58\n",
      "Total tag matches: 59\n",
      "Total tag matches: 60\n",
      "Total tag matches: 61\n",
      "Total tag matches: 62\n",
      "Total tag matches: 63\n",
      "Total tag matches: 64\n",
      "Total tag matches: 65\n",
      "Total tag matches: 66\n",
      "Total tag matches: 67\n",
      "Total tag matches: 68\n",
      "Total tag matches: 69\n",
      "kl loss/word=0.0024\n",
      "Total tag matches: 70\n",
      "Total tag matches: 71\n",
      "Total tag matches: 72\n",
      "Total tag matches: 73\n",
      "Total tag matches: 74\n",
      "Total tag matches: 75\n",
      "Total tag matches: 76\n",
      "kl loss/word=0.0014\n",
      "Total tag matches: 77\n",
      "Total tag matches: 78\n",
      "Total tag matches: 79\n",
      "Total tag matches: 80\n",
      "Total tag matches: 81\n",
      "Total tag matches: 82\n",
      "Total tag matches: 83\n",
      "Total tag matches: 84\n",
      "Total tag matches: 85\n",
      "Total tag matches: 86\n",
      "kl loss/word=0.0019\n",
      "Total tag matches: 87\n",
      "Total tag matches: 88\n",
      "Total tag matches: 89\n",
      "Total tag matches: 90\n",
      "Total tag matches: 91\n",
      "Total tag matches: 92\n",
      "Total tag matches: 93\n",
      "Total tag matches: 94\n",
      "Total tag matches: 95\n",
      "Total tag matches: 96\n",
      "Total tag matches: 97\n",
      "Total tag matches: 98\n",
      "kl loss/word=0.0035\n",
      "Total tag matches: 99\n",
      "Total tag matches: 100\n",
      "Total tag matches: 101\n",
      "Total tag matches: 102\n",
      "Total tag matches: 103\n",
      "kl loss/word=0.0019\n",
      "Total tag matches: 104\n",
      "Total tag matches: 105\n",
      "Total tag matches: 106\n",
      "Total tag matches: 107\n",
      "Total tag matches: 108\n",
      "Total tag matches: 109\n",
      "kl loss/word=0.0021\n",
      "Total tag matches: 110\n",
      "Total tag matches: 111\n",
      "Total tag matches: 112\n",
      "Total tag matches: 113\n",
      "Total tag matches: 114\n",
      "Total tag matches: 115\n",
      "Total tag matches: 116\n",
      "Total tag matches: 117\n",
      "Total tag matches: 118\n",
      "Total tag matches: 119\n",
      "kl loss/word=0.0025\n",
      "Total tag matches: 120\n",
      "Total tag matches: 121\n",
      "Total tag matches: 122\n",
      "Total tag matches: 123\n",
      "Total tag matches: 124\n",
      "Total tag matches: 125\n",
      "Total tag matches: 126\n",
      "Total tag matches: 127\n",
      "Total tag matches: 128\n",
      "Total tag matches: 129\n",
      "Total tag matches: 130\n",
      "Total tag matches: 131\n",
      "Total tag matches: 132\n",
      "kl loss/word=0.0020\n",
      "Total tag matches: 133\n",
      "Total tag matches: 134\n",
      "Total tag matches: 135\n",
      "Total tag matches: 137\n",
      "Total tag matches: 138\n",
      "Total tag matches: 139\n",
      "Total tag matches: 140\n",
      "Total tag matches: 141\n",
      "Total tag matches: 142\n",
      "Total tag matches: 143\n",
      "kl loss/word=0.0030\n",
      "Total tag matches: 144\n",
      "Total tag matches: 145\n",
      "Total tag matches: 146\n",
      "Total tag matches: 147\n",
      "Total tag matches: 148\n",
      "Total tag matches: 149\n",
      "Total tag matches: 150\n",
      "Total tag matches: 151\n",
      "Total tag matches: 152\n",
      "Total tag matches: 153\n",
      "Total tag matches: 154\n",
      "Total tag matches: 155\n",
      "Total tag matches: 156\n",
      "kl loss/word=0.0022\n",
      "Total tag matches: 157\n",
      "Total tag matches: 158\n",
      "Total tag matches: 159\n",
      "Total tag matches: 160\n",
      "Total tag matches: 161\n",
      "Total tag matches: 162\n",
      "Total tag matches: 163\n",
      "Total tag matches: 164\n",
      "Total tag matches: 165\n",
      "Total tag matches: 166\n",
      "Total tag matches: 167\n",
      "Total tag matches: 168\n",
      "Total tag matches: 169\n",
      "Total tag matches: 170\n",
      "kl loss/word=0.0024\n",
      "Total tag matches: 171\n",
      "Total tag matches: 172\n",
      "Total tag matches: 173\n",
      "Total tag matches: 174\n",
      "Total tag matches: 175\n",
      "Total tag matches: 176\n",
      "Total tag matches: 177\n",
      "Total tag matches: 178\n",
      "Total tag matches: 179\n",
      "Total tag matches: 180\n",
      "kl loss/word=0.0021\n",
      "Total tag matches: 181\n",
      "Total tag matches: 182\n",
      "Total tag matches: 183\n",
      "Total tag matches: 184\n",
      "Total tag matches: 185\n",
      "Total tag matches: 186\n",
      "Total tag matches: 187\n",
      "Total tag matches: 188\n",
      "Total tag matches: 189\n",
      "Total tag matches: 190\n",
      "Total tag matches: 191\n",
      "Total tag matches: 192\n",
      "Total tag matches: 193\n",
      "kl loss/word=0.0027\n",
      "Total tag matches: 194\n",
      "Total tag matches: 195\n",
      "Total tag matches: 196\n",
      "Total tag matches: 197\n",
      "Total tag matches: 198\n",
      "Total tag matches: 199\n",
      "Total tag matches: 200\n",
      "Total tag matches: 201\n",
      "Total tag matches: 202\n",
      "Total tag matches: 203\n",
      "Total tag matches: 204\n",
      "Total tag matches: 205\n",
      "Total tag matches: 206\n",
      "Total tag matches: 207\n",
      "Total tag matches: 208\n",
      "Total tag matches: 209\n",
      "Total tag matches: 210\n",
      "Total tag matches: 211\n",
      "Total tag matches: 212\n",
      "Total tag matches: 213\n",
      "Total tag matches: 214\n",
      "Total tag matches: 215\n",
      "Total tag matches: 217\n",
      "Total tag matches: 218\n",
      "Total tag matches: 219\n",
      "kl loss/word=0.0033\n",
      "Total tag matches: 220\n",
      "Total tag matches: 221\n",
      "Total tag matches: 222\n",
      "Total tag matches: 223\n",
      "Total tag matches: 224\n",
      "Total tag matches: 225\n",
      "Total tag matches: 226\n",
      "Total tag matches: 227\n",
      "Total tag matches: 228\n",
      "Total tag matches: 229\n",
      "kl loss/word=0.0019\n",
      "Total tag matches: 230\n",
      "Total tag matches: 231\n",
      "Total tag matches: 232\n",
      "Total tag matches: 233\n",
      "Total tag matches: 234\n",
      "Total tag matches: 235\n",
      "Total tag matches: 236\n",
      "Total tag matches: 237\n",
      "Total tag matches: 238\n",
      "Total tag matches: 239\n",
      "Total tag matches: 240\n",
      "Total tag matches: 241\n",
      "Total tag matches: 242\n",
      "kl loss/word=0.0030\n",
      "Total tag matches: 243\n",
      "Total tag matches: 244\n",
      "Total tag matches: 245\n",
      "Total tag matches: 246\n",
      "Total tag matches: 247\n",
      "Total tag matches: 248\n",
      "Total tag matches: 249\n",
      "Total tag matches: 250\n",
      "Total tag matches: 251\n",
      "Total tag matches: 252\n",
      "Total tag matches: 253\n",
      "Total tag matches: 254\n",
      "Total tag matches: 255\n",
      "Total tag matches: 256\n",
      "Total tag matches: 257\n",
      "Total tag matches: 258\n",
      "Total tag matches: 260\n",
      "Total tag matches: 261\n",
      "Total tag matches: 262\n",
      "Total tag matches: 263\n",
      "Total tag matches: 264\n",
      "Total tag matches: 265\n",
      "kl loss/word=0.0015\n",
      "Total tag matches: 266\n",
      "Total tag matches: 267\n",
      "Total tag matches: 268\n",
      "Total tag matches: 269\n",
      "Total tag matches: 270\n",
      "Total tag matches: 271\n",
      "Total tag matches: 272\n",
      "Total tag matches: 273\n",
      "Total tag matches: 274\n",
      "Total tag matches: 275\n",
      "\n",
      "k=5\tprecision: 0.002449997772729298\trecall: 0.009154082587197647\tf1: 0.0038654475416189686\n"
     ]
    }
   ],
   "source": [
    "# Get evaluations on training\n",
    "\n",
    "eval_k = {} # {k: (prec, recall, f1)}\n",
    "\n",
    "train_samp = random.sample(train, len(dev))\n",
    "\n",
    "for k in range(1,6):\n",
    "    eval_k[k] = evaluate(train_samp, k)\n",
    "    print(\"k={}\\tprecision: {}\\trecall: {}\\tf1: {}\".format(k, eval_k[k][0], eval_k[k][1], eval_k[k][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25817"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25817"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precisions = [p for p,_,_ in eval_k.values()]\n",
    "recalls = [r for _,r,_ in eval_k.values()]\n",
    "f1s = [f for _,_,f in eval_k.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(precisions, eval_k.keys(), '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Takes a long time\n",
    "train_toks = [[str(w) for w in t][:MAX_LEN] for t in nlp.pipe([x.encode('ascii', 'ignore').decode('ascii').lower() for x in twitter_texts], n_threads=3, batch_size=20000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212855"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/usr2/mamille2/twitter/data/huang2016_data/train_toks.pkl', 'wb') as f:\n",
    "    pickle.dump(train_toks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Takes a long time\n",
    "dev_toks = [[str(w) for w in t][:MAX_LEN] for t in nlp.pipe([x.encode('ascii', 'ignore').decode('ascii').lower() for x in dev_texts], n_threads=3, batch_size=20000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/usr2/mamille2/twitter/data/huang2016_data/dev_toks.pkl', 'wb') as f:\n",
    "    pickle.dump(dev_toks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_toks = [[str(w) for w in t][:MAX_LEN] for t in nlp.pipe([x.encode('ascii', 'ignore').decode('ascii').lower() for x in test_texts], n_threads=3, batch_size=20000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/usr2/mamille2/twitter/data/huang2016_data/test_toks.pkl', 'wb') as f:\n",
    "    pickle.dump(test_toks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['rt',\n",
       "  '@walmsley_liz',\n",
       "  ':',\n",
       "  'morning',\n",
       "  'becky',\n",
       "  ',',\n",
       "  'you',\n",
       "  'might',\n",
       "  'not',\n",
       "  'be',\n",
       "  'surprised',\n",
       "  'to',\n",
       "  'know',\n",
       "  'that',\n",
       "  'we',\n",
       "  'are',\n",
       "  'going',\n",
       "  'for',\n",
       "  'a',\n",
       "  'walk',\n",
       "  ',',\n",
       "  'recce',\n",
       "  'for',\n",
       "  'while',\n",
       "  ' ',\n",
       "  'https'],\n",
       " ['young',\n",
       "  ',',\n",
       "  'african',\n",
       "  'scientists',\n",
       "  'inspiring',\n",
       "  'next',\n",
       "  'peer',\n",
       "  'group',\n",
       "  'of',\n",
       "  'innovators',\n",
       "  '@empower_women',\n",
       "  ' ',\n",
       "  'https://t.co/6apzob0ds2'],\n",
       " ['in',\n",
       "  'tech',\n",
       "  '-',\n",
       "  'paypal',\n",
       "  'nixes',\n",
       "  'purchase',\n",
       "  'protection',\n",
       "  'for',\n",
       "  'payments',\n",
       "  'made',\n",
       "  'through',\n",
       "  'crowdfunding',\n",
       "  'platforms',\n",
       "  'https://t.co/ikf7gvunvq']]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_toks[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_CAP = 10 ** 5\n",
    "VOCAB_CAP = 50000\n",
    "# VOCAB_CAP = len(word_counts)\n",
    "# VOCAB_CAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "special_stops = [c for c in string.punctuation] + \\\n",
    "                    ['amp', ' ', 'rt', '\\n', '\\n\\n', 'https://t', 'https://t.c', 'https://t.co', '...']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCounting words ... 356658 unique types, 49997 restricted vocab"
     ]
    }
   ],
   "source": [
    "print ('\\tCounting words ... ', end='')\n",
    "word_counts = defaultdict(int)\n",
    "for t in train_toks:\n",
    "    for x in t:\n",
    "        if not x in special_stops:\n",
    "            word_counts[x] += 1\n",
    "        \n",
    "top_k_words = set(sorted(word_counts, key=word_counts.get, reverse=True)[:VOCAB_CAP-3])\n",
    "print(len(word_counts.keys()), end=' unique types, ')\n",
    "print(len(top_k_words), end=' restricted vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'to',\n",
       " 'a',\n",
       " 'in',\n",
       " 'of',\n",
       " 'for',\n",
       " 'and',\n",
       " 'on',\n",
       " 'is',\n",
       " 'you',\n",
       " \"'s\",\n",
       " 'i',\n",
       " 'with',\n",
       " 'it',\n",
       " 'this',\n",
       " 'at',\n",
       " 'your',\n",
       " 'by',\n",
       " 'from',\n",
       " 'are',\n",
       " 'we',\n",
       " 'via',\n",
       " 'that',\n",
       " 'my',\n",
       " 'be',\n",
       " 'new',\n",
       " 'out',\n",
       " 'how',\n",
       " 'do',\n",
       " 'have',\n",
       " 'our',\n",
       " 'what',\n",
       " \"n't\",\n",
       " 'all',\n",
       " 'not',\n",
       " 'will',\n",
       " 'more',\n",
       " 'about',\n",
       " 'can',\n",
       " 'as',\n",
       " 'up',\n",
       " 'an',\n",
       " 'now',\n",
       " 'just',\n",
       " 'get',\n",
       " 'if',\n",
       " 'one',\n",
       " 'today',\n",
       " 'so',\n",
       " 'me',\n",
       " 'time',\n",
       " 'has',\n",
       " 'but',\n",
       " 'here',\n",
       " 'was',\n",
       " 'day',\n",
       " 'us',\n",
       " 'who',\n",
       " 'great',\n",
       " 'https',\n",
       " 'love',\n",
       " 'like',\n",
       " 'they',\n",
       " 'when',\n",
       " 'no',\n",
       " '2',\n",
       " 'good',\n",
       " 'or',\n",
       " 'follow',\n",
       " 'why',\n",
       " 'he',\n",
       " 's',\n",
       " 'see',\n",
       " 'his',\n",
       " '1',\n",
       " 'best',\n",
       " 'people',\n",
       " 'their',\n",
       " 'world',\n",
       " 'back',\n",
       " 'make',\n",
       " 'over',\n",
       " 'check',\n",
       " 'need',\n",
       " 'there',\n",
       " 'after',\n",
       " 'help',\n",
       " '  ',\n",
       " '2016',\n",
       " 'first',\n",
       " '3',\n",
       " 'know',\n",
       " 'go',\n",
       " 'free',\n",
       " 'some',\n",
       " '4',\n",
       " '5',\n",
       " '..',\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " 'thanks',\n",
       " 'year',\n",
       " 'want',\n",
       " 'than',\n",
       " 'should',\n",
       " 'last',\n",
       " 'its',\n",
       " 'live',\n",
       " 'win',\n",
       " 'work',\n",
       " 'week',\n",
       " 'https://',\n",
       " 'happy',\n",
       " 'life',\n",
       " 'did',\n",
       " 'https:/',\n",
       " 'only',\n",
       " 'may',\n",
       " 'into',\n",
       " '10',\n",
       " 'http',\n",
       " 'been',\n",
       " 'her',\n",
       " 'right',\n",
       " 'top',\n",
       " 'read',\n",
       " 'next',\n",
       " 'way',\n",
       " 'these',\n",
       " 'take',\n",
       " 'most',\n",
       " 'off',\n",
       " 'says',\n",
       " 'video',\n",
       " 'could',\n",
       " 'thank',\n",
       " 'would',\n",
       " 'big',\n",
       " 'does',\n",
       " 'them',\n",
       " 'look',\n",
       " 'news',\n",
       " 'tonight',\n",
       " 'game',\n",
       " 'watch',\n",
       " 'home',\n",
       " 'ca',\n",
       " 'still',\n",
       " 'business',\n",
       " 'got',\n",
       " 'find',\n",
       " 'think',\n",
       " 'night',\n",
       " 'let',\n",
       " 'use',\n",
       " 'down',\n",
       " 'looking',\n",
       " 'too',\n",
       " 'going',\n",
       " 'join',\n",
       " 'htt',\n",
       " 'much',\n",
       " 'show',\n",
       " 'please',\n",
       " 'years',\n",
       " 'come',\n",
       " 'social',\n",
       " 'women',\n",
       " 'team',\n",
       " 'w/',\n",
       " 'support',\n",
       " 'never',\n",
       " 'where',\n",
       " 'being',\n",
       " 'morning',\n",
       " '6',\n",
       " 'were',\n",
       " 'every',\n",
       " 'had',\n",
       " \"'ve\",\n",
       " 'nt',\n",
       " 'very',\n",
       " 'https://t.co/',\n",
       " 'media',\n",
       " 'better',\n",
       " 'against',\n",
       " 'am',\n",
       " 'things',\n",
       " 'say',\n",
       " 'she',\n",
       " 'must',\n",
       " 'really',\n",
       " 'made',\n",
       " 'learn',\n",
       " 'well',\n",
       " 'man',\n",
       " 'm',\n",
       " '7',\n",
       " '\\n ',\n",
       " \"'ll\",\n",
       " 'start',\n",
       " 'many',\n",
       " 'ht',\n",
       " 'change',\n",
       " 'u',\n",
       " 'city',\n",
       " 'ever',\n",
       " 'everyone',\n",
       " 'season',\n",
       " 'book',\n",
       " 'another',\n",
       " 'before',\n",
       " 'story',\n",
       " 'any',\n",
       " 'real',\n",
       " 'tips',\n",
       " 'part',\n",
       " ':)',\n",
       " 'other',\n",
       " 'amazing',\n",
       " 'vote',\n",
       " 'pm',\n",
       " 'always',\n",
       " 'followers',\n",
       " 'beautiful',\n",
       " 'again',\n",
       " 'two',\n",
       " 'open',\n",
       " 'end',\n",
       " 'future',\n",
       " 'stop',\n",
       " 'ready',\n",
       " 'him',\n",
       " 'give',\n",
       " 'old',\n",
       " 'little',\n",
       " 'twitter',\n",
       " 'keep',\n",
       " 'data',\n",
       " 'report',\n",
       " 'weekend',\n",
       " 'high',\n",
       " '2015',\n",
       " 'w',\n",
       " 'through',\n",
       " 'even',\n",
       " 'getting',\n",
       " 'tomorrow',\n",
       " 'h',\n",
       " 'latest',\n",
       " 'post',\n",
       " 'state',\n",
       " 'set',\n",
       " 'days',\n",
       " 'long',\n",
       " 'because',\n",
       " 'family',\n",
       " 'listen',\n",
       " 'coming',\n",
       " 'play',\n",
       " 'april',\n",
       " 'visit',\n",
       " 'trump',\n",
       " 'those',\n",
       " 'retweet',\n",
       " 'ways',\n",
       " 'music',\n",
       " '8',\n",
       " 'then',\n",
       " 'making',\n",
       " 'house',\n",
       " 'hope',\n",
       " 'full',\n",
       " 'photo',\n",
       " 'marketing',\n",
       " 'online',\n",
       " 'power',\n",
       " 'health',\n",
       " 'talk',\n",
       " 'friends',\n",
       " 'share',\n",
       " 'fun',\n",
       " 'meet',\n",
       " 'own',\n",
       " 'following',\n",
       " 'president',\n",
       " 'sure',\n",
       " 'since',\n",
       " 'million',\n",
       " 'money',\n",
       " 'food',\n",
       " 'away',\n",
       " 'fans',\n",
       " 'job',\n",
       " '50',\n",
       " 'which',\n",
       " 'blog',\n",
       " 'run',\n",
       " 'during',\n",
       " 'call',\n",
       " 'using',\n",
       " '....',\n",
       " 'needs',\n",
       " 'awesome',\n",
       " 'school',\n",
       " 'place',\n",
       " 'kids',\n",
       " 'event',\n",
       " '100',\n",
       " '20',\n",
       " 'around',\n",
       " 'stories',\n",
       " 'party',\n",
       " 'uk',\n",
       " 'while',\n",
       " 'thing',\n",
       " '\\n\\n ',\n",
       " 'working',\n",
       " 'done',\n",
       " 'global',\n",
       " 'makes',\n",
       " 'doing',\n",
       " 'black',\n",
       " '9',\n",
       " 'de',\n",
       " 'miss',\n",
       " 'something',\n",
       " 'without',\n",
       " 'save',\n",
       " 'nice',\n",
       " 'chance',\n",
       " 'tell',\n",
       " 'also',\n",
       " '0',\n",
       " 'gt',\n",
       " 'try',\n",
       " 'enter',\n",
       " 'deal',\n",
       " '12',\n",
       " 'yes',\n",
       " '1st',\n",
       " 'national',\n",
       " 'summer',\n",
       " 'vs',\n",
       " 'easy',\n",
       " 'between',\n",
       " 'google',\n",
       " 'bad',\n",
       " 'app',\n",
       " 'art',\n",
       " 'police',\n",
       " 'children',\n",
       " 'series',\n",
       " 'special',\n",
       " 'said',\n",
       " 'looks',\n",
       " 'yet',\n",
       " 'same',\n",
       " 'march',\n",
       " 't',\n",
       " 'feel',\n",
       " 're',\n",
       " 'watching',\n",
       " 'design',\n",
       " 'found',\n",
       " 'fight',\n",
       " 'congrats',\n",
       " '11',\n",
       " 'everything',\n",
       " 'service',\n",
       " 'final',\n",
       " '15',\n",
       " 'important',\n",
       " 'times',\n",
       " 'white',\n",
       " 'water',\n",
       " 'favorite',\n",
       " 'plan',\n",
       " 'heart',\n",
       " 'list',\n",
       " 'wait',\n",
       " 'true',\n",
       " 'london',\n",
       " 'guide',\n",
       " 'sign',\n",
       " 'nothing',\n",
       " 'welcome',\n",
       " 'sales',\n",
       " 'project',\n",
       " 'experience',\n",
       " 'hard',\n",
       " 'friday',\n",
       " 'public',\n",
       " 'young',\n",
       " 'gets',\n",
       " 'god',\n",
       " 'students',\n",
       " 'spring',\n",
       " 'put',\n",
       " '--',\n",
       " 'remember',\n",
       " 'left',\n",
       " 'care',\n",
       " 'until',\n",
       " 'available',\n",
       " 'face',\n",
       " 'case',\n",
       " 'bill',\n",
       " 'few',\n",
       " 'forward',\n",
       " '30',\n",
       " 'group',\n",
       " 'info',\n",
       " 'stay',\n",
       " 'company',\n",
       " 'market',\n",
       " 'perfect',\n",
       " 'anyone',\n",
       " 'campaign',\n",
       " 'under',\n",
       " 'hey',\n",
       " 'proud',\n",
       " 'three',\n",
       " 'action',\n",
       " 'behind',\n",
       " 'mobile',\n",
       " 'lead',\n",
       " 'government',\n",
       " 'review',\n",
       " 'gain',\n",
       " 'community',\n",
       " 'red',\n",
       " 'study',\n",
       " 'someone',\n",
       " 'create',\n",
       " 'd',\n",
       " 'country',\n",
       " 'success',\n",
       " 'jobs',\n",
       " 'u.s',\n",
       " 'america',\n",
       " 'building',\n",
       " 'together',\n",
       " 'history',\n",
       " 'soon',\n",
       " 'gt;&gt',\n",
       " 'digital',\n",
       " 'star',\n",
       " 'tech',\n",
       " 'content',\n",
       " 'taking',\n",
       " 'small',\n",
       " 'update',\n",
       " 'obama',\n",
       " 'ago',\n",
       " 'cool',\n",
       " 'key',\n",
       " 'photos',\n",
       " 'head',\n",
       " 'won',\n",
       " 'killed',\n",
       " 'bring',\n",
       " 'hit',\n",
       " 'talking',\n",
       " 'tv',\n",
       " 'men',\n",
       " 'hear',\n",
       " 'wo',\n",
       " 'tweet',\n",
       " 'super',\n",
       " 'seen',\n",
       " 'woman',\n",
       " 'build',\n",
       " 'lost',\n",
       " 'local',\n",
       " 'goal',\n",
       " 'might',\n",
       " 'having',\n",
       " 'girl',\n",
       " 'oh',\n",
       " 'energy',\n",
       " 'hot',\n",
       " 'rights',\n",
       " 'baby',\n",
       " 'industry',\n",
       " 'fast',\n",
       " 'line',\n",
       " 'human',\n",
       " 'early',\n",
       " 'takes',\n",
       " 'less',\n",
       " 'yourself',\n",
       " 'pay',\n",
       " 'light',\n",
       " 'sunday',\n",
       " 'far',\n",
       " 'monday',\n",
       " 'shows',\n",
       " 'become',\n",
       " 'interview',\n",
       " 'security',\n",
       " 'excited',\n",
       " '@justinbieber',\n",
       " 'development',\n",
       " 'believe',\n",
       " 'fire',\n",
       " 'enjoy',\n",
       " 'used',\n",
       " 'others',\n",
       " 'war',\n",
       " 'worth',\n",
       " 'office',\n",
       " 'research',\n",
       " '25',\n",
       " 'american',\n",
       " 'facebook',\n",
       " 'green',\n",
       " 'comes',\n",
       " 'meeting',\n",
       " 'playing',\n",
       " 'apple',\n",
       " 'interesting',\n",
       " 'park',\n",
       " 'john',\n",
       " 'each',\n",
       " 'training',\n",
       " 'eu',\n",
       " 'search',\n",
       " 'games',\n",
       " 'already',\n",
       " 'both',\n",
       " 'wants',\n",
       " 'enough',\n",
       " 'daily',\n",
       " 'move',\n",
       " 'finally',\n",
       " 'second',\n",
       " 'goes',\n",
       " 'international',\n",
       " 'death',\n",
       " 'record',\n",
       " 'n',\n",
       " 'words',\n",
       " 'road',\n",
       " 'release',\n",
       " 'friend',\n",
       " 'breaking',\n",
       " 'lot',\n",
       " 'leaders',\n",
       " 'running',\n",
       " 'ask',\n",
       " 'month',\n",
       " 'e',\n",
       " 'guys',\n",
       " 'view',\n",
       " 'wow',\n",
       " 'buy',\n",
       " 'hours',\n",
       " 'moment',\n",
       " 'b',\n",
       " 'brand',\n",
       " 'growth',\n",
       " ' \\n',\n",
       " 'court',\n",
       " 'st',\n",
       " 'based',\n",
       " 'else',\n",
       " 'shot',\n",
       " 'blue',\n",
       " 'r',\n",
       " 'trying',\n",
       " 'song',\n",
       " 'dead',\n",
       " 'huge',\n",
       " 'saturday',\n",
       " 'web',\n",
       " 'winning',\n",
       " 'article',\n",
       " 'award',\n",
       " \"'d\",\n",
       " 'sale',\n",
       " 'program',\n",
       " '16',\n",
       " 'birthday',\n",
       " 'travel',\n",
       " 'conference',\n",
       " 'girls',\n",
       " 'yesterday',\n",
       " 'person',\n",
       " 'reading',\n",
       " 'site',\n",
       " 'beat',\n",
       " 'car',\n",
       " 'mind',\n",
       " 'law',\n",
       " 'gift',\n",
       " 'thinking',\n",
       " 'ahead',\n",
       " 'x',\n",
       " 'stand',\n",
       " 'race',\n",
       " 'strong',\n",
       " '13',\n",
       " 'starting',\n",
       " 'air',\n",
       " 'united',\n",
       " 'ad',\n",
       " 'close',\n",
       " 'giving',\n",
       " 'official',\n",
       " 'congratulations',\n",
       " 'started',\n",
       " 'film',\n",
       " 'reasons',\n",
       " 'past',\n",
       " 'companies',\n",
       " 'name',\n",
       " 'learning',\n",
       " 'child',\n",
       " 'south',\n",
       " 'turn',\n",
       " 'system',\n",
       " 'services',\n",
       " 'hello',\n",
       " 'level',\n",
       " 'short',\n",
       " 'trip',\n",
       " 'pick',\n",
       " 'leader',\n",
       " 'class',\n",
       " 'performance',\n",
       " 'forget',\n",
       " 'almost',\n",
       " 'street',\n",
       " 'canada',\n",
       " 'climate',\n",
       " 'half',\n",
       " 'such',\n",
       " 'point',\n",
       " 'details',\n",
       " 'single',\n",
       " 'fall',\n",
       " 'oil',\n",
       " 'india',\n",
       " 'talks',\n",
       " 'word',\n",
       " 'website',\n",
       " 'step',\n",
       " 'ideas',\n",
       " 'members',\n",
       " 'minutes',\n",
       " 'states',\n",
       " 'round',\n",
       " 'peace',\n",
       " 'york',\n",
       " 'v',\n",
       " 'low',\n",
       " 'trade',\n",
       " 'grow',\n",
       " 'north',\n",
       " 'mean',\n",
       " 'gold',\n",
       " 'education',\n",
       " 'self',\n",
       " 'wins',\n",
       " 'challenge',\n",
       " 'mother',\n",
       " 'impact',\n",
       " 'risk',\n",
       " 'improve',\n",
       " 'hillary',\n",
       " 'steps',\n",
       " 'fan',\n",
       " 'ft',\n",
       " 'pls',\n",
       " 'retweeted',\n",
       " 'four',\n",
       " 'pretty',\n",
       " 'space',\n",
       " 'lives',\n",
       " 'wrong',\n",
       " 'writing',\n",
       " 'five',\n",
       " 'former',\n",
       " 'direction',\n",
       " 'manager',\n",
       " 'customer',\n",
       " 'c',\n",
       " 'break',\n",
       " 'europe',\n",
       " 'dream',\n",
       " 'thought',\n",
       " 'strategy',\n",
       " 'near',\n",
       " 'room',\n",
       " 'access',\n",
       " 'iphone',\n",
       " 'once',\n",
       " 'living',\n",
       " 'increase',\n",
       " 'plans',\n",
       " 'attack',\n",
       " 'major',\n",
       " 'phone',\n",
       " 'opportunity',\n",
       " 'side',\n",
       " 'episode',\n",
       " 'biggest',\n",
       " 'matter',\n",
       " 'west',\n",
       " 'needed',\n",
       " 'wind',\n",
       " 'page',\n",
       " 'waiting',\n",
       " 'order',\n",
       " 'calls',\n",
       " 'lol',\n",
       " 'podcast',\n",
       " 'books',\n",
       " 'sharing',\n",
       " 'winner',\n",
       " 'test',\n",
       " 'link',\n",
       " 'anything',\n",
       " '2nd',\n",
       " 'network',\n",
       " 'add',\n",
       " 'starts',\n",
       " 'store',\n",
       " 'listening',\n",
       " 'internet',\n",
       " 'tax',\n",
       " 'china',\n",
       " 'en',\n",
       " 'justin',\n",
       " 'walk',\n",
       " 'km',\n",
       " 'club',\n",
       " 'late',\n",
       " 'radio',\n",
       " 'awards',\n",
       " 'cc',\n",
       " 'icymi',\n",
       " 'heard',\n",
       " '14',\n",
       " 'drive',\n",
       " 'anti',\n",
       " 'ceo',\n",
       " 'clinton',\n",
       " 'actually',\n",
       " 'inside',\n",
       " 'policy',\n",
       " 'issue',\n",
       " 'problem',\n",
       " 'called',\n",
       " 'press',\n",
       " 'tour',\n",
       " 'rise',\n",
       " 'economy',\n",
       " 'la',\n",
       " 'technology',\n",
       " 'cup',\n",
       " 'hour',\n",
       " 'card',\n",
       " 'value',\n",
       " 'focus',\n",
       " 'election',\n",
       " 'ur',\n",
       " 'management',\n",
       " 'came',\n",
       " 'piece',\n",
       " '@realdonaldtrump',\n",
       " 'different',\n",
       " 'rules',\n",
       " 'lovely',\n",
       " 'questions',\n",
       " 'co',\n",
       " 'countries',\n",
       " 'science',\n",
       " 'mt',\n",
       " 'across',\n",
       " 'na',\n",
       " 'register',\n",
       " 'results',\n",
       " 'missing',\n",
       " 'leave',\n",
       " 'reports',\n",
       " 'minister',\n",
       " 'o',\n",
       " 'simple',\n",
       " 'political',\n",
       " 'center',\n",
       " 'opening',\n",
       " 'david',\n",
       " 'act',\n",
       " 'took',\n",
       " 'smart',\n",
       " 'least',\n",
       " 'control',\n",
       " 'price',\n",
       " 'celebrate',\n",
       " 'fantastic',\n",
       " 'issues',\n",
       " 'amazon',\n",
       " 'tickets',\n",
       " 'months',\n",
       " 'met',\n",
       " 'crisis',\n",
       " 'thursday',\n",
       " 'league',\n",
       " 'sun',\n",
       " 'number',\n",
       " 'went',\n",
       " 'wednesday',\n",
       " 'idea',\n",
       " 'pro',\n",
       " 'prince',\n",
       " 'clean',\n",
       " 'ice',\n",
       " 'beach',\n",
       " 'kind',\n",
       " 'hi',\n",
       " 'town',\n",
       " 'gives',\n",
       " 'means',\n",
       " 'growing',\n",
       " 'age',\n",
       " 'write',\n",
       " 'un',\n",
       " 'workers',\n",
       " 'per',\n",
       " 'bank',\n",
       " 'front',\n",
       " 'role',\n",
       " 'weeks',\n",
       " 'question',\n",
       " 'thoughts',\n",
       " 'trends',\n",
       " 'hand',\n",
       " 'bernie',\n",
       " 'weather',\n",
       " 'career',\n",
       " 'safe',\n",
       " 'advice',\n",
       " 'code',\n",
       " 'protect',\n",
       " 'wonderful',\n",
       " 'ends',\n",
       " 'guy',\n",
       " 'launch',\n",
       " 'mom',\n",
       " 'skills',\n",
       " 'album',\n",
       " 'loved',\n",
       " 'boy',\n",
       " 'goals',\n",
       " 'earth',\n",
       " 'area',\n",
       " 'outside',\n",
       " 'shop',\n",
       " 'course',\n",
       " 'http://t',\n",
       " 'calling',\n",
       " '18',\n",
       " 'understand',\n",
       " 'changing',\n",
       " 'bit',\n",
       " 'innovation',\n",
       " 'events',\n",
       " 'artist',\n",
       " 'healthy',\n",
       " 'hold',\n",
       " '   ',\n",
       " 'director',\n",
       " 'reason',\n",
       " 'match',\n",
       " 'dog',\n",
       " 'helping',\n",
       " 'style',\n",
       " 'maybe',\n",
       " 'tools',\n",
       " 'michael',\n",
       " 'dr',\n",
       " 'writers',\n",
       " 'successful',\n",
       " 'rock',\n",
       " 'wish',\n",
       " '40',\n",
       " '2014',\n",
       " 'gon',\n",
       " 'users',\n",
       " 'non',\n",
       " 'apps',\n",
       " 'cover',\n",
       " 'cut',\n",
       " 'sweet',\n",
       " 'cost',\n",
       " 'whole',\n",
       " 'author',\n",
       " 'hate',\n",
       " 'fear',\n",
       " 'hotel',\n",
       " '@onedirection',\n",
       " 'catch',\n",
       " 'reality',\n",
       " 'track',\n",
       " 'al',\n",
       " 'reach',\n",
       " 'africa',\n",
       " 'voting',\n",
       " 'powerful',\n",
       " 'download',\n",
       " 'justice',\n",
       " 'boost',\n",
       " '21',\n",
       " 'board',\n",
       " 'beauty',\n",
       " 'king',\n",
       " 'pic',\n",
       " 'solution',\n",
       " '@wef',\n",
       " 'voice',\n",
       " '24',\n",
       " 'email',\n",
       " 'cloud',\n",
       " 'billion',\n",
       " 'signed',\n",
       " 'ok',\n",
       " 'lets',\n",
       " 'tells',\n",
       " 'freedom',\n",
       " 'happen',\n",
       " 'process',\n",
       " 'lots',\n",
       " 'football',\n",
       " 'truth',\n",
       " 'mark',\n",
       " 'feeling',\n",
       " 'planning',\n",
       " 'govt',\n",
       " 'player',\n",
       " 'products',\n",
       " 'return',\n",
       " 'shares',\n",
       " 'field',\n",
       " 'european',\n",
       " 'tuesday',\n",
       " 'possible',\n",
       " 'body',\n",
       " 'eat',\n",
       " 'votes',\n",
       " 'due',\n",
       " 'trust',\n",
       " 'traffic',\n",
       " 'journey',\n",
       " 'double',\n",
       " 'customers',\n",
       " 'economic',\n",
       " 'offers',\n",
       " 'wanted',\n",
       " 'answer',\n",
       " 'movie',\n",
       " 'told',\n",
       " 'hands',\n",
       " 'picture',\n",
       " 'families',\n",
       " 'incredible',\n",
       " 'secret',\n",
       " 'crazy',\n",
       " 'student',\n",
       " 'session',\n",
       " 'yours',\n",
       " 'drop',\n",
       " 'vs.',\n",
       " 'agree',\n",
       " 'die',\n",
       " 'budget',\n",
       " 'chris',\n",
       " 'quality',\n",
       " 'island',\n",
       " 'quick',\n",
       " 'selling',\n",
       " 'exclusive',\n",
       " 'luck',\n",
       " 'moving',\n",
       " 'stage',\n",
       " 'instagram',\n",
       " 'continues',\n",
       " 'senate',\n",
       " 'lose',\n",
       " 'send',\n",
       " 'leadership',\n",
       " 'lady',\n",
       " 'click',\n",
       " 'donald',\n",
       " 'finds',\n",
       " 'evening',\n",
       " 'chat',\n",
       " 'land',\n",
       " 'guess',\n",
       " ...]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove special characters, others\n",
    "sorted(word_counts, key=word_counts.get, reverse=True)[:VOCAB_CAP-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_unk_texts = [' '.join(['<S>'] + [w if w in top_k_words else '<UNK>' for w in t] + ['</S>']) for t in train_toks]\n",
    "dev_unk_texts = [' '.join(['<S>'] + [w if w in top_k_words else '<UNK>' for w in t] + ['</S>']) for t in dev_toks]\n",
    "test_unk_texts = [' '.join(['<S>'] + [w if w in top_k_words else '<UNK>' for w in t] + ['</S>']) for t in test_toks]\n",
    "\n",
    "train_dev_unk_texts = train_unk_texts + dev_unk_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212855\n",
      "212855\n",
      "185190\n",
      "185190\n"
     ]
    }
   ],
   "source": [
    "# Filter out no-tag instances\n",
    "\n",
    "print(len(train_unk_texts))\n",
    "print(len(parsed_tags))\n",
    "\n",
    "train_filtered_texts = []\n",
    "train_filtered_tags = []\n",
    "\n",
    "for i in range(len(train_unk_texts)):\n",
    "    if len(train_unk_texts[i].split()) >= 3 and len(parsed_tags[i]) != 0:\n",
    "        train_filtered_texts.append(train_unk_texts[i])\n",
    "        train_filtered_tags.append(parsed_tags[i])\n",
    "        \n",
    "print(len(train_filtered_texts))\n",
    "print(len(train_filtered_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25817\n",
      "25817\n",
      "22431\n",
      "22431\n"
     ]
    }
   ],
   "source": [
    "# Filter out no-tag instances\n",
    "\n",
    "print(len(dev_unk_texts))\n",
    "print(len(dev_tags))\n",
    "\n",
    "dev_filtered_texts = []\n",
    "dev_filtered_tags = []\n",
    "\n",
    "for i in range(len(dev_unk_texts)):\n",
    "    if len(dev_unk_texts[i].split()) >= 3 and len(parsed_tags[i]) != 0:\n",
    "        dev_filtered_texts.append(dev_unk_texts[i])\n",
    "        dev_filtered_tags.append(parsed_tags[i])\n",
    "        \n",
    "print(len(dev_filtered_texts))\n",
    "print(len(dev_filtered_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19614\n",
      "19614\n",
      "17046\n",
      "17046\n"
     ]
    }
   ],
   "source": [
    "# Filter out no-tag instances\n",
    "\n",
    "print(len(test_unk_texts))\n",
    "print(len(test_tags))\n",
    "\n",
    "test_filtered_texts = []\n",
    "test_filtered_tags = []\n",
    "\n",
    "for i in range(len(test_unk_texts)):\n",
    "    if len(test_unk_texts[i].split()) >= 3 and len(parsed_tags[i]) != 0:\n",
    "        test_filtered_texts.append(test_unk_texts[i])\n",
    "        test_filtered_tags.append(parsed_tags[i])\n",
    "        \n",
    "print(len(test_filtered_texts))\n",
    "print(len(test_filtered_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207621"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dev_filtered_texts = train_filtered_texts + dev_filtered_texts\n",
    "train_dev_filtered_tags = train_filtered_tags + dev_filtered_tags\n",
    "\n",
    "len(train_dev_filtered_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185190"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = CountVectorizer(min_df=1, stop_words='english')\n",
    "# v.fit(train_dev_filtered_texts)\n",
    "v.fit(train_filtered_texts)\n",
    "# v.fit(test_filtered_texts)\n",
    "v.fit(dev_filtered_texts)\n",
    "\n",
    "# bow_train_dev = v.transform(train_dev_filtered_texts)\n",
    "bow_train = v.transform(train_filtered_texts)\n",
    "# bow_test = v.transform(test_filtered_texts)\n",
    "bow_dev = v.transform(dev_filtered_texts)\n",
    "bow_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(185190,)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_top_tags = np.array([random.sample(t,1) for t in train_filtered_tags]).ravel()\n",
    "train_top_tags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22431,)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_top_tags = np.array([random.sample(t,1) for t in dev_filtered_tags]).ravel()\n",
    "dev_top_tags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(207621,)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dev_top_tags = np.array([random.sample(t,1) for t in train_dev_filtered_tags]).ravel()\n",
    "train_dev_top_tags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17046,)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_top_tags = np.array([random.sample(t,1) for t in test_filtered_tags]).ravel()\n",
    "test_top_tags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_X = bow_train_dev\n",
    "train_X = bow_train\n",
    "train_y = train_top_tags\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@1: 0.007450428252962572\n"
     ]
    }
   ],
   "source": [
    "# quick k=1 check\n",
    "\n",
    "# preds = clf.predict(bow_test)\n",
    "preds = clf.predict(bow_dev)\n",
    "\n",
    "# matches = sum(preds==test_top_tags)\n",
    "# matches = sum(1 for i in range(len(test_filtered_tags)) if preds[i] in test_filtered_tags[i])\n",
    "matches = sum(1 for i in range(len(dev_filtered_tags)) if preds[i] in dev_filtered_tags[i])\n",
    "\n",
    "print('Precision@1: {}'.format(matches/len(test_filtered_tags)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = list(zip(parsed_texts, parsed_tags))\n",
    "dev_tags = index_tags(dev_tags, tag_set, tag_indexes)\n",
    "dev = list(zip(parsed_dev_texts, dev_tags))\n",
    "test_tags = index_tags(test_tags, tag_set, tag_indexes)\n",
    "test = list(zip(parsed_test_texts, test_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reverse index lookup\n",
    "def vocab2idx(i):\n",
    "    return list(vocab.keys())[list(vocab.values()).index(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reverse index lookup\n",
    "def idx2tag(i):\n",
    "    return list(vocab.keys())[list(vocab.values()).index(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S> rt <UNK> : morning <UNK> , you might not be surprised to know that we are going for a walk , <UNK> for while   https </S>\n",
      "coaching walking\n",
      "\n",
      "<S> young , african scientists inspiring next peer group of innovators <UNK>   <UNK> </S>\n",
      "women Diversity\n",
      "\n",
      "<S> in tech - paypal <UNK> purchase protection for payments made through crowdfunding platforms <UNK> </S>\n",
      "Toronto Social\n",
      "\n",
      "<S> good morning   <UNK> ! happy <UNK> day ! </S>\n",
      "FollowBack\n",
      "\n",
      "<S> florida trip guides thank you for following ! visit https://t.co/pl9kbaltxt to get a </S>\n",
      "free eBook\n",
      "\n",
      "<S> rt <UNK> : for all the out there ! this is a good read ! <UNK> </S>\n",
      "bloggers\n",
      "\n",
      "<S> rt <UNK> : in 2016 , and other will cost the nation $ <UNK> billion , cost will <UNK> $ 1 trillion by 2050 . htt </S>\n",
      "Alzheimers dementia\n",
      "\n",
      "<S> life <UNK> <UNK> </S>\n",
      "inspiration\n",
      "\n",
      "<S> how much is star wars really worth to disney ? <UNK>   via @alerttrade <UNK> </S>\n",
      "News\n",
      "\n",
      "<S> mostly sunny today ! with a high of <UNK> and a low of <UNK> . </S>\n",
      "Teamfollowback\n",
      "\n",
      "<S> imagine if they 'd brought some tall skinny <UNK> onto the happy days set & amp ; said , \" this is the new <UNK> ! \" only 1 <UNK> </S>\n",
      "TopGear\n",
      "\n",
      "<S> it 's what i wish for you ... a life with no regrets . - <UNK> <UNK> </S>\n",
      "quote\n",
      "\n",
      "<S> rt <UNK> : thanks to education workers for fighting bill <UNK> . bill was ill <UNK> & amp ; <UNK> with charter rights as confirmed by co </S>\n",
      "\n",
      "\n",
      "<S> <UNK> i believe the <UNK> information <UNK> given to in order to get a small fine . we must dig more </S>\n",
      "SFO\n",
      "\n",
      "<S> rt <UNK> : <UNK> : <UNK> library for beautiful drawing <UNK> <UNK> </S>\n",
      "javascript\n",
      "\n",
      "<S> rt <UNK> : what problem do you solve ... \n",
      "\n",
      " better than anyone on earth ? \n",
      "\n",
      " and ... \n",
      "\n",
      " who knows it ? \n",
      "\n",
      " </S>\n",
      "fb\n",
      "\n",
      "<S> how can we make <UNK> recommendations relevant <UNK>   learn more with our <UNK> <UNK> <UNK> </S>\n",
      "UofT\n",
      "\n",
      "<S> rt <UNK> : ' planet # 2   the <UNK>   in <UNK> ' <UNK> </S>\n",
      "EXO\n",
      "\n",
      "<S> rt <UNK> : with <UNK> <UNK> team in 2015 . <UNK> works with communities to eliminate the threat of mines . ht </S>\n",
      "Cyprus\n",
      "\n",
      "<S> rt <UNK> : how can cruz 's \" <UNK> them all & amp ; never let them back in \" stance , win a general election ? </S>\n",
      "TNTweeters immigration\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t_idx in range(20):\n",
    "    print(' '.join([idx_to_vocab[i] for i in train[t_idx][0]]))\n",
    "    print(' '.join([idx_to_tag[i] for i in train[t_idx][1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215118"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load original data, check\n",
    "train_orig = pd.read_pickle('/usr2/mamille2/twitter/data/huang2016_data/huang2016_train.pkl')\n",
    "len(train_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>lang</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>user_name</th>\n",
       "      <th>text</th>\n",
       "      <th>text_no_tags</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>727062271499886592</td>\n",
       "      <td>Mon May 02 09:08:48 +0000 2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>3</td>\n",
       "      <td>122475762</td>\n",
       "      <td>janathe9s</td>\n",
       "      <td>Jan Floyd-Douglass</td>\n",
       "      <td>RT @walmsley_liz: morning Becky, you might not be surprised to know that we are going for a walk, recce for #coaching while #walking. https…</td>\n",
       "      <td>RT @walmsley_liz: morning Becky, you might not be surprised to know that we are going for a walk, recce for while  https…</td>\n",
       "      <td>[coaching, walking]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>720745760141094912</td>\n",
       "      <td>Thu Apr 14 22:49:14 +0000 2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>119434726</td>\n",
       "      <td>IbisComm</td>\n",
       "      <td>Ibis Communications</td>\n",
       "      <td>Young, African #women scientists inspiring next peer group of innovators #Diversity @Empower_Women  https://t.co/6aPzOB0Ds2</td>\n",
       "      <td>Young, African scientists inspiring next peer group of innovators @Empower_Women  https://t.co/6aPzOB0Ds2</td>\n",
       "      <td>[women, Diversity]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>729553156162478080</td>\n",
       "      <td>Mon May 09 06:06:41 +0000 2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>121853287</td>\n",
       "      <td>BoatingCanada</td>\n",
       "      <td>Boating Canada</td>\n",
       "      <td>In Tech - PayPal nixes Purchase Protection for payments made through crowdfunding platforms https://t.co/Ikf7GvunVQ #Toronto #Social</td>\n",
       "      <td>In Tech - PayPal nixes Purchase Protection for payments made through crowdfunding platforms https://t.co/Ikf7GvunVQ</td>\n",
       "      <td>[Toronto, Social]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>697462116404609024</td>\n",
       "      <td>Wed Feb 10 16:48:21 +0000 2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>115527525</td>\n",
       "      <td>melindadiane912</td>\n",
       "      <td>Melinda ♓️</td>\n",
       "      <td>Good morning ⛅️💋 @CeejayHuncho! Happy hump day 🐪! #FollowBack👌</td>\n",
       "      <td>Good morning ⛅️💋 @CeejayHuncho! Happy hump day 🐪!</td>\n",
       "      <td>[FollowBack]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>713551944770527237</td>\n",
       "      <td>Sat Mar 26 02:23:35 +0000 2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>117473213</td>\n",
       "      <td>BetteLeeCrosby</td>\n",
       "      <td>Bette Lee Crosby</td>\n",
       "      <td>Florida Trip Guides thank you for following! Visit https://t.co/pL9kbaLtxt to get a #free #eBook</td>\n",
       "      <td>Florida Trip Guides thank you for following! Visit https://t.co/pL9kbaLtxt to get a</td>\n",
       "      <td>[free, eBook]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                      created_at  in_reply_to_status_id  \\\n",
       "0  727062271499886592  Mon May 02 09:08:48 +0000 2016                    NaN   \n",
       "1  720745760141094912  Thu Apr 14 22:49:14 +0000 2016                    NaN   \n",
       "2  729553156162478080  Mon May 09 06:06:41 +0000 2016                    NaN   \n",
       "3  697462116404609024  Wed Feb 10 16:48:21 +0000 2016                    NaN   \n",
       "4  713551944770527237  Sat Mar 26 02:23:35 +0000 2016                    NaN   \n",
       "\n",
       "  lang  retweet_count    user_id user_screen_name            user_name  \\\n",
       "0   en              3  122475762        janathe9s   Jan Floyd-Douglass   \n",
       "1   en              0  119434726         IbisComm  Ibis Communications   \n",
       "2   en              0  121853287    BoatingCanada       Boating Canada   \n",
       "3   en              1  115527525  melindadiane912           Melinda ♓️   \n",
       "4   en              0  117473213   BetteLeeCrosby     Bette Lee Crosby   \n",
       "\n",
       "                                                                                                                                           text  \\\n",
       "0  RT @walmsley_liz: morning Becky, you might not be surprised to know that we are going for a walk, recce for #coaching while #walking. https…   \n",
       "1                   Young, African #women scientists inspiring next peer group of innovators #Diversity @Empower_Women  https://t.co/6aPzOB0Ds2   \n",
       "2          In Tech - PayPal nixes Purchase Protection for payments made through crowdfunding platforms https://t.co/Ikf7GvunVQ #Toronto #Social   \n",
       "3                                                                                Good morning ⛅️💋 @CeejayHuncho! Happy hump day 🐪! #FollowBack👌   \n",
       "4                                              Florida Trip Guides thank you for following! Visit https://t.co/pL9kbaLtxt to get a #free #eBook   \n",
       "\n",
       "                                                                                                                text_no_tags  \\\n",
       "0  RT @walmsley_liz: morning Becky, you might not be surprised to know that we are going for a walk, recce for while  https…   \n",
       "1                  Young, African scientists inspiring next peer group of innovators @Empower_Women  https://t.co/6aPzOB0Ds2   \n",
       "2       In Tech - PayPal nixes Purchase Protection for payments made through crowdfunding platforms https://t.co/Ikf7GvunVQ    \n",
       "3                                                                         Good morning ⛅️💋 @CeejayHuncho! Happy hump day 🐪!    \n",
       "4                                       Florida Trip Guides thank you for following! Visit https://t.co/pL9kbaLtxt to get a    \n",
       "\n",
       "                  tags  \n",
       "0  [coaching, walking]  \n",
       "1   [women, Diversity]  \n",
       "2    [Toronto, Social]  \n",
       "3         [FollowBack]  \n",
       "4        [free, eBook]  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    RT @walmsley_liz: morning Becky, you might not be surprised to know that we are going for a walk, recce for #coaching while #walking. https…\n",
       "1                     Young, African #women scientists inspiring next peer group of innovators #Diversity @Empower_Women  https://t.co/6aPzOB0Ds2\n",
       "2            In Tech - PayPal nixes Purchase Protection for payments made through crowdfunding platforms https://t.co/Ikf7GvunVQ #Toronto #Social\n",
       "3                                                                                  Good morning ⛅️💋 @CeejayHuncho! Happy hump day 🐪! #FollowBack👌\n",
       "4                                                Florida Trip Guides thank you for following! Visit https://t.co/pL9kbaLtxt to get a #free #eBook\n",
       "5                                              RT @timetravlblonde: For all the #bloggers out there! This is a good read! https://t.co/VAzg3KdFuU\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_orig.loc[:5, 'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorize to one-hot bag-of-word vectors, 100 in length\n",
    "def bow_vec(tweet_inds):\n",
    "    vec = np.zeros(VOCAB_SIZE)\n",
    "    for idx in tweet_inds:\n",
    "        vec[idx] += 1\n",
    "        \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['young',\n",
       "  ',',\n",
       "  'african',\n",
       "  'scientists',\n",
       "  'inspiring',\n",
       "  'next',\n",
       "  'peer',\n",
       "  'group',\n",
       "  'of',\n",
       "  'innovators',\n",
       "  '<UNK>',\n",
       "  ' ',\n",
       "  '<UNK>'],\n",
       " ['women', 'Diversity'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [idx_to_vocab[i] for i in train[1][0]], [idx_to_tag[i] for i in train[1][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<S>',\n",
       "  'states',\n",
       "  'working',\n",
       "  'on',\n",
       "  'new',\n",
       "  'accounts',\n",
       "  'for',\n",
       "  'disabled',\n",
       "  'families',\n",
       "  ':',\n",
       "  'see',\n",
       "  '<UNK>',\n",
       "  'story',\n",
       "  '<UNK>',\n",
       "  '</S>'],\n",
       " ['pharma'],\n",
       " ['FF',\n",
       "  'news',\n",
       "  'Pakistan',\n",
       "  'weddings',\n",
       "  'GetGlueHD',\n",
       "  'socialmedia',\n",
       "  'giveaway',\n",
       "  'Entrepreneur',\n",
       "  'startup',\n",
       "  'Facebook'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 1000\n",
    "pred = hallucinate_tags(train[idx][0])\n",
    "pred_args = np.argsort(pred)[::-1]\n",
    "[idx_to_vocab[i] for i in train[idx][0]], [idx_to_tag[i] for i in train[idx][1]], [idx_to_tag[i] for i in pred_args[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.795193672180176e-05, 9.557604789733887e-05)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[3239], pred[165]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 246,  147,  113, ..., 1875,  774, 1093])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hallucinate_tweet(given_tags):\n",
    "    dy.renew_cg()\n",
    "    \n",
    "    # Transduce all batch elements with an LSTM\n",
    "    tags = given_tags\n",
    "\n",
    "    # initialize the LSTM\n",
    "    #init_state_src = lstm_encode.initial_state()\n",
    "\n",
    "    # get the output of the first LSTM\n",
    "    #src_output = init_state_src.add_inputs([embed[x] for x in src])[-1].output()\n",
    "\n",
    "    # Now compute mean and standard deviation of source hidden state.\n",
    "    #W_mu_tweet = dy.parameter(W_mu_tweet_p)\n",
    "    #V_mu_tweet = dy.parameter(V_mu_tweet_p)\n",
    "    #b_mu_tweet = dy.parameter(b_mu_tweet_p)\n",
    "\n",
    "    #W_sig_tweet = dy.parameter(W_sig_tweet_p)\n",
    "    #V_sig_tweet = dy.parameter(V_sig_tweet_p)\n",
    "    #b_sig_tweet = dy.parameter(b_sig_tweet_p)\n",
    "    \n",
    "    # Compute tweet encoding\n",
    "    #mu_tweet      = mlp(src_output, W_mu_tweet,  V_mu_tweet,  b_mu_tweet)\n",
    "    #log_var_tweet = mlp(src_output, W_sig_tweet, V_sig_tweet, b_sig_tweet)\n",
    "    \n",
    "    W_mu_tag = dy.parameter(W_mu_tag_p)\n",
    "    V_mu_tag = dy.parameter(V_mu_tag_p)\n",
    "    b_mu_tag = dy.parameter(b_mu_tag_p)\n",
    "\n",
    "    W_sig_tag = dy.parameter(W_sig_tag_p)\n",
    "    V_sig_tag = dy.parameter(V_sig_tag_p)\n",
    "    b_sig_tag = dy.parameter(b_sig_tag_p)\n",
    "    \n",
    "    # Compute tag encoding\n",
    "    tags_tensor = dy.sparse_inputTensor([tags], np.ones((len(tags),)), (NUM_TAGS,))\n",
    "    \n",
    "    mu_tag      = dy.dropout(mlp(tags_tensor, W_mu_tag,  V_mu_tag,  b_mu_tag), DROPOUT)\n",
    "    log_var_tag = dy.dropout(mlp(tags_tensor, W_sig_tag, V_sig_tag, b_sig_tag), DROPOUT)\n",
    "    \n",
    "    # Combine encodings for mean and diagonal covariance\n",
    "    W_mu = dy.parameter(W_mu_p)\n",
    "    b_mu = dy.parameter(b_mu_p)\n",
    "\n",
    "    W_sig = dy.parameter(W_sig_p)\n",
    "    b_sig = dy.parameter(b_sig_p)\n",
    "    \n",
    "    mu_tweet = dy.zeros(HIDDEN_DIM)\n",
    "    log_var_tweet = dy.zeros(HIDDEN_DIM)\n",
    "    \n",
    "    mu      = dy.affine_transform([b_mu,  W_mu,  dy.concatenate([mu_tweet, mu_tag])])\n",
    "    log_var = dy.affine_transform([b_sig, W_sig, dy.concatenate([log_var_tweet, log_var_tag])])\n",
    "\n",
    "    # KL-Divergence loss computation\n",
    "    kl_loss = -0.5 * dy.sum_elems(1 + log_var - dy.pow(mu, dy.inputVector([2])) - dy.exp(log_var))\n",
    "\n",
    "    z = reparameterize(mu, log_var)\n",
    "\n",
    "    # now step through the output sentence\n",
    "    all_losses = []\n",
    "\n",
    "    current_state = lstm_decode.initial_state().set_s([z, dy.tanh(z)])\n",
    "    prev_word = vocab[START]\n",
    "    W_sm = dy.parameter(W_tweet_softmax_p)\n",
    "    b_sm = dy.parameter(b_tweet_softmax_p)\n",
    "\n",
    "    gen_tweet = []\n",
    "    for i in range(20):\n",
    "        # feed the current state into the\n",
    "        current_state = current_state.add_input(embed[prev_word])\n",
    "        output_embedding = current_state.output()\n",
    "\n",
    "        s = dy.affine_transform([b_sm, W_sm, output_embedding])\n",
    "        p = dy.softmax(s).npvalue()\n",
    "        next_word = np.random.choice(VOCAB_SIZE, p=p/p.sum())\n",
    "        gen_tweet.append(next_word)\n",
    "        prev_word = next_word\n",
    "                               \n",
    "    return gen_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['savings'],\n",
       " ['shanghai',\n",
       "  '<UNK>',\n",
       "  'rt',\n",
       "  'platforms',\n",
       "  'htt',\n",
       "  'surprised',\n",
       "  'board',\n",
       "  'get',\n",
       "  'days',\n",
       "  'set',\n",
       "  '&',\n",
       "  '5-star',\n",
       "  'rt',\n",
       "  'ed',\n",
       "  'rt',\n",
       "  '@justintrudeau',\n",
       "  'in',\n",
       "  'jumped',\n",
       "  'rt',\n",
       "  'bear'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 1000\n",
    "[idx_to_tag[i] for i in train[idx][1]], [idx_to_vocab[i] for i in hallucinate_tweet(train[idx][1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<S>',\n",
       "  'states',\n",
       "  'working',\n",
       "  'on',\n",
       "  'new',\n",
       "  'accounts',\n",
       "  'for',\n",
       "  'disabled',\n",
       "  'families',\n",
       "  ':',\n",
       "  'see',\n",
       "  '<UNK>',\n",
       "  'story',\n",
       "  '<UNK>',\n",
       "  '</S>'],\n",
       " ['Ankara'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 1000\n",
    "[idx_to_vocab[i] for i in train[idx][0]], [idx_to_tag[i] for i in hallucinate_tags(train[idx][0])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
