{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import spacy\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle\n",
    "\n",
    "import dynet_config\n",
    "dynet_config.set(mem=1024, random_seed=12345)\n",
    "dynet_config.set_gpu()\n",
    "import dynet as dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spaCy\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading spaCy\")\n",
    "nlp = spacy.load('en')\n",
    "assert nlp.path is not None\n",
    "print ('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 100\n",
    "NUM_TAGS = 3883\n",
    "VOCAB_CAP = 10000\n",
    "\n",
    "UNK = '<UNK>'\n",
    "START = '<S>'\n",
    "END = '</S>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data for Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'huang2016_train.aligned.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-30c41986a916>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'huang2016_train.aligned.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtwitter_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtwitter_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtwitter_histories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'huang2016_valid.aligned.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdev_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_histories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'huang2016_train.aligned.pkl'"
     ]
    }
   ],
   "source": [
    "with open('huang2016_train.aligned.pkl', 'rb') as f:\n",
    "    twitter_texts, twitter_tags, twitter_histories = pickle.load(f)\n",
    "    \n",
    "with open('huang2016_valid.aligned.pkl', 'rb') as f:\n",
    "    dev_texts, dev_tags, dev_histories = pickle.load(f)\n",
    "    \n",
    "with open('huang2016_test.aligned.pkl', 'rb') as f:\n",
    "    test_texts, test_tags, test_histories = pickle.load(f)\n",
    "    \n",
    "del twitter_histories, dev_histories, test_histories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def index_tags(tags_list, tag_set, tag_dict):\n",
    "    return [[tag_dict[tag] for tag in tags if tag in tag_set] for tags in tags_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3883 unique tags.\n"
     ]
    }
   ],
   "source": [
    "# Extract tag set\n",
    "tag_counts = defaultdict(int)\n",
    "for t in twitter_tags:\n",
    "    for x in t:\n",
    "        tag_counts[x] += 1\n",
    "top_k_tags = set(sorted(tag_counts, key=tag_counts.get, reverse=True)[:NUM_TAGS])\n",
    "\n",
    "tag_set = set()\n",
    "for t in twitter_tags:\n",
    "    tag_set.update(set([x for x in t if x in top_k_tags]))\n",
    "    \n",
    "tag_set = sorted(tag_set)\n",
    "print ('{} unique tags.'.format(len(tag_set)))\n",
    "\n",
    "tag_indexes = defaultdict(lambda: len(tag_indexes))\n",
    "parsed_tags = index_tags(twitter_tags, tag_set, tag_indexes)\n",
    "idx_to_tag = {v: k for k, v in tag_indexes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to open preprecessed TRAIN data ... DONE. (0.590s)\n",
      "Vocab size: 10000\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print ('Attempting to open preprecessed TRAIN data ... ', end='')\n",
    "    #raise NotImplemented\n",
    "    \n",
    "    t0=time()\n",
    "    with open('parsed_twitter_train_data_no_histories.pkl', 'rb') as f:\n",
    "        vocab, parsed_texts, parsed_tags = pickle.load(f)\n",
    "    print ('DONE. ({:.3f}s)'.format(time()-t0))\n",
    "        \n",
    "except:\n",
    "    print ('FAIL.')\n",
    "    \n",
    "    print ('\\tParsing texts ... ', end='')\n",
    "    t0=time()\n",
    "    parsed_texts = [[str(w) for w in t][:MAX_LEN] for t in nlp.pipe([x.encode('ascii', 'ignore').decode('ascii').lower() for x in twitter_texts], n_threads=3, batch_size=20000)]\n",
    "    print ('DONE. ({:.3f}s)'.format(time()-t0))\n",
    "    \n",
    "    print ('\\tCounting words ... ', end='')\n",
    "    word_counts = defaultdict(int)\n",
    "    for t in parsed_texts:\n",
    "        for x in t:\n",
    "            word_counts[x] += 1\n",
    "    top_k_words = set(sorted(word_counts, key=word_counts.get, reverse=True)[:VOCAB_CAP-3])\n",
    "\n",
    "    word_set = set()\n",
    "    for t in parsed_texts:\n",
    "        word_set.update(set([x for x in t if x in top_k_words]))\n",
    "    print ('DONE. ({:.3f}s)'.format(time()-t0)) \n",
    "    \n",
    "    vocab = defaultdict(lambda: len(vocab))\n",
    "    print ('\\tIndexing texts ... ', end='')\n",
    "    t0=time()\n",
    "    parsed_texts = [[vocab[START]] + [(vocab[w] if w in word_set else vocab[UNK]) for w in t] + [vocab[END]] for t in parsed_texts]\n",
    "    print ('DONE. ({:.3f}s)'.format(time()-t0))\n",
    "    \n",
    "    unk_idx = vocab[UNK]\n",
    "    sos_idx = vocab[START]\n",
    "    eos_idx = vocab[END]\n",
    "    \n",
    "    print ('\\tSAVING parsed data ... ', end='')\n",
    "    t0=time()\n",
    "    with open('parsed_twitter_train_data_no_histories.pkl', 'wb') as f:\n",
    "        pickle.dump((dict(vocab), parsed_texts, parsed_tags), f) \n",
    "    print ('DONE. ({:.3f}s)'.format(time()-t0))\n",
    "\n",
    "unk_idx = vocab[UNK]\n",
    "sos_idx = vocab[START]\n",
    "eos_idx = vocab[END]\n",
    "# Set unknown words to be UNK --> note as written, the paper does not indicate that any training data is labeled as UNK...\n",
    "vocab = defaultdict(lambda: unk_idx, vocab)\n",
    "idx_to_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "print ('Vocab size:', VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to open preprecessed DEV and TEST data ... DONE. (0.033s)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print ('Attempting to open preprecessed DEV and TEST data ... ', end='')\n",
    "    #raise NotImplemented\n",
    "    \n",
    "    t0=time()\n",
    "    with open('parsed_twitter_test_dev_data_no_histories.pkl', 'rb') as f:\n",
    "        parsed_dev_texts, parsed_test_texts = pickle.load(f)\n",
    "    print ('DONE. ({:.3f}s)'.format(time()-t0))\n",
    "        \n",
    "except:\n",
    "    print ('FAIL.')\n",
    "    print ('\\tParsing texts ... ', end='')\n",
    "    t0=time()\n",
    "    parsed_dev_texts = [[vocab[START]] + [vocab[str(w)] for w in t if not w.is_stop][:MAX_LEN] + [vocab[END]] for t in nlp.pipe([x.encode('ascii', 'ignore').decode('ascii').lower() for x in dev_texts], n_threads=3, batch_size=20000)]\n",
    "    parsed_test_texts = [[vocab[START]] + [vocab[str(w)] for w in t if not w.is_stop][:MAX_LEN] + [vocab[END]] for t in nlp.pipe([x.encode('ascii', 'ignore').decode('ascii').lower() for x in test_texts], n_threads=3, batch_size=20000)]\n",
    "    print ('DONE. ({:.3f}s)'.format(time()-t0))\n",
    "    \n",
    "    print ('\\tSAVING parsed data ... ', end='')\n",
    "    t0=time()\n",
    "    with open('parsed_twitter_test_dev_data_no_histories.pkl', 'wb') as f:\n",
    "        pickle.dump((parsed_dev_texts, parsed_test_texts), f) \n",
    "    print ('DONE. ({:.3f}s)'.format(time()-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "Q_DIM = 512\n",
    "DROPOUT = 0.2\n",
    "ALPHA = 0.01\n",
    "EPSILON_MAX = .9\n",
    "EPSILON_MIN = 0.00\n",
    "KL_WEIGHT_START = 0.0\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "PATIENCE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dynet model\n",
    "model = dy.ParameterCollection()\n",
    "\n",
    "# The paper uses AdaGrad\n",
    "trainer = dy.AdamTrainer(model)\n",
    "\n",
    "# Embedding parameters\n",
    "embed = model.add_lookup_parameters((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "# Recurrent layers for tweet encoding\n",
    "lstm_encode = dy.LSTMBuilder(1, EMBEDDING_DIM, HIDDEN_DIM, model)\n",
    "lstm_decode = dy.LSTMBuilder(1, EMBEDDING_DIM, Q_DIM, model)\n",
    "\n",
    "# Encoder MLP for tweet encoding\n",
    "W_mu_tweet_p = model.add_parameters((Q_DIM, HIDDEN_DIM))\n",
    "V_mu_tweet_p = model.add_parameters((HIDDEN_DIM, Q_DIM))\n",
    "b_mu_tweet_p = model.add_parameters((Q_DIM))\n",
    "\n",
    "W_sig_tweet_p = model.add_parameters((Q_DIM, HIDDEN_DIM))\n",
    "V_sig_tweet_p = model.add_parameters((HIDDEN_DIM, Q_DIM))\n",
    "b_sig_tweet_p = model.add_parameters((Q_DIM))\n",
    "\n",
    "W_mu_tag_p = model.add_parameters((Q_DIM, NUM_TAGS))\n",
    "V_mu_tag_p = model.add_parameters((HIDDEN_DIM, Q_DIM))\n",
    "b_mu_tag_p = model.add_parameters((Q_DIM))\n",
    "\n",
    "W_sig_tag_p = model.add_parameters((Q_DIM, NUM_TAGS))\n",
    "V_sig_tag_p = model.add_parameters((HIDDEN_DIM, Q_DIM))\n",
    "b_sig_tag_p = model.add_parameters((Q_DIM))\n",
    "\n",
    "W_mu_p = model.add_parameters((Q_DIM, 2 * HIDDEN_DIM))\n",
    "b_mu_p = model.add_parameters((Q_DIM))\n",
    "\n",
    "W_sig_p = model.add_parameters((Q_DIM, 2 * HIDDEN_DIM))\n",
    "b_sig_p = model.add_parameters((Q_DIM))\n",
    "\n",
    "W_hidden_p = model.add_parameters((HIDDEN_DIM, Q_DIM))\n",
    "b_hidden_p = model.add_parameters((HIDDEN_DIM))\n",
    "\n",
    "W_tweet_softmax_p = model.add_parameters((VOCAB_SIZE, Q_DIM))\n",
    "b_tweet_softmax_p = model.add_parameters((VOCAB_SIZE))\n",
    "\n",
    "W_tag_output_p = model.add_parameters((NUM_TAGS, HIDDEN_DIM))\n",
    "b_tag_output_p = model.add_parameters((NUM_TAGS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reparameterize(mu, log_sigma_squared):\n",
    "    d = mu.dim()[0][0]\n",
    "    sample = dy.random_normal(d)\n",
    "    covar = dy.exp(log_sigma_squared * 0.5)\n",
    "\n",
    "    return mu + dy.cmult(covar, sample)\n",
    "\n",
    "def mlp(x, W, V, b):\n",
    "    return V * dy.tanh(W * x + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(sent, epsilon=0.0):\n",
    "    #dy.renew_cg()\n",
    "    \n",
    "    \n",
    "\n",
    "    # Transduce all batch elements with an LSTM\n",
    "    src = sent[0]\n",
    "    tags = sent[1]\n",
    "\n",
    "    # initialize the LSTM\n",
    "    init_state_src = lstm_encode.initial_state()\n",
    "\n",
    "    # get the output of the first LSTM\n",
    "    src_output = init_state_src.add_inputs([embed[x] for x in src])[-1].output()\n",
    "\n",
    "    # Now compute mean and standard deviation of source hidden state.\n",
    "    W_mu_tweet = dy.parameter(W_mu_tweet_p)\n",
    "    V_mu_tweet = dy.parameter(V_mu_tweet_p)\n",
    "    b_mu_tweet = dy.parameter(b_mu_tweet_p)\n",
    "\n",
    "    W_sig_tweet = dy.parameter(W_sig_tweet_p)\n",
    "    V_sig_tweet = dy.parameter(V_sig_tweet_p)\n",
    "    b_sig_tweet = dy.parameter(b_sig_tweet_p)\n",
    "    \n",
    "    # Compute tweet encoding\n",
    "    mu_tweet      = dy.dropout(mlp(src_output, W_mu_tweet,  V_mu_tweet,  b_mu_tweet), DROPOUT)\n",
    "    log_var_tweet = dy.dropout(mlp(src_output, W_sig_tweet, V_sig_tweet, b_sig_tweet), DROPOUT)\n",
    "    \n",
    "    W_mu_tag = dy.parameter(W_mu_tag_p)\n",
    "    V_mu_tag = dy.parameter(V_mu_tag_p)\n",
    "    b_mu_tag = dy.parameter(b_mu_tag_p)\n",
    "\n",
    "    W_sig_tag = dy.parameter(W_sig_tag_p)\n",
    "    V_sig_tag = dy.parameter(V_sig_tag_p)\n",
    "    b_sig_tag = dy.parameter(b_sig_tag_p)\n",
    "    \n",
    "    # Compute tag encoding\n",
    "    tags_tensor = dy.sparse_inputTensor([tags], np.ones((len(tags),)), (NUM_TAGS,))\n",
    "    \n",
    "    mu_tag      = dy.dropout(mlp(tags_tensor, W_mu_tag,  V_mu_tag,  b_mu_tag), DROPOUT)\n",
    "    log_var_tag = dy.dropout(mlp(tags_tensor, W_sig_tag, V_sig_tag, b_sig_tag), DROPOUT)\n",
    "    \n",
    "    # Combine encodings for mean and diagonal covariance\n",
    "    W_mu = dy.parameter(W_mu_p)\n",
    "    b_mu = dy.parameter(b_mu_p)\n",
    "\n",
    "    W_sig = dy.parameter(W_sig_p)\n",
    "    b_sig = dy.parameter(b_sig_p)\n",
    "    \n",
    "    # Slowly phase out getting both inputs\n",
    "    if random.random() < epsilon:\n",
    "        mask = dy.zeros(HIDDEN_DIM)\n",
    "    else:\n",
    "        mask = dy.ones(HIDDEN_DIM)\n",
    "        \n",
    "    if random.random() < 0.5:\n",
    "        mu_tweet = dy.cmult(mu_tweet, mask)\n",
    "        log_var_tweet = dy.cmult(log_var_tweet, mask)\n",
    "    else:\n",
    "        mu_tag = dy.cmult(mu_tag, mask)\n",
    "        log_var_tag = dy.cmult(log_var_tag, mask)\n",
    "    \n",
    "    mu      = dy.affine_transform([b_mu,  W_mu,  dy.concatenate([mu_tweet, mu_tag])])\n",
    "    log_var = dy.affine_transform([b_sig, W_sig, dy.concatenate([log_var_tweet, log_var_tag])])\n",
    "\n",
    "    # KL-Divergence loss computation\n",
    "    kl_loss = -0.5 * dy.sum_elems(1 + log_var - dy.pow(mu, dy.inputVector([2])) - dy.exp(log_var))\n",
    "\n",
    "    z = reparameterize(mu, log_var)\n",
    "\n",
    "    # now step through the output sentence\n",
    "    all_losses = []\n",
    "\n",
    "    current_state = lstm_decode.initial_state().set_s([z, dy.tanh(z)])\n",
    "    prev_word = src[0]\n",
    "    W_sm = dy.parameter(W_tweet_softmax_p)\n",
    "    b_sm = dy.parameter(b_tweet_softmax_p)\n",
    "\n",
    "    for next_word in src[1:]:\n",
    "        # feed the current state into the\n",
    "        \n",
    "        \n",
    "        current_state = current_state.add_input(embed[prev_word])\n",
    "        output_embedding = current_state.output()\n",
    "\n",
    "        s = dy.affine_transform([b_sm, W_sm, output_embedding])\n",
    "        \n",
    "        all_losses.append(dy.pickneglogsoftmax(s, next_word))\n",
    "\n",
    "        # Slowly phase out teacher forcing (this may be slow??)\n",
    "        if random.random() < epsilon:\n",
    "            p = dy.softmax(s).npvalue()\n",
    "            prev_word = np.random.choice(VOCAB_SIZE, p=p/p.sum())\n",
    "        else:\n",
    "            prev_word = next_word\n",
    "    \n",
    "    softmax_loss = dy.esum(all_losses)\n",
    "\n",
    "    W_hidden = dy.parameter(W_hidden_p)\n",
    "    b_hidden = dy.parameter(b_hidden_p)\n",
    "    \n",
    "    W_out = dy.parameter(W_tag_output_p)\n",
    "    b_out = dy.parameter(b_tag_output_p)\n",
    "    \n",
    "    h = dy.dropout(dy.tanh(b_hidden + W_hidden * z), DROPOUT)\n",
    "    o = dy.logistic(b_out + W_out * h)\n",
    "    \n",
    "    crossentropy_loss = dy.binary_log_loss(o, tags_tensor)\n",
    "                               \n",
    "    return kl_loss, softmax_loss, crossentropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = list(zip(parsed_texts, parsed_tags))\n",
    "dev_tags = index_tags(dev_tags, tag_set, tag_indexes)\n",
    "dev = list(zip(parsed_dev_texts, dev_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/13304 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch size of 16.\n",
      "Training ... Iteration: 0 Epsilon: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13304/13304 [1:31:24<00:00,  2.43it/s]\n",
      "  0%|          | 7/25817 [00:00<06:42, 64.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss/word=0.3311, kl loss/word=0.8457, reconstruction loss/word=4.3154, ppl=1.3925, tag loss=13.8994s\n",
      "Evaluating batch ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25817/25817 [06:11<00:00, 69.58it/s]\n",
      "  0%|          | 0/13304 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: dev loss/word=5.6335, kl loss/word=0.1024, reconstruction loss/word=4.6220, ppl=279.6253, tag loss=10.81s\n",
      "Training ... Iteration: 1 Epsilon: 0.022367912868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13304/13304 [1:30:50<00:00,  2.44it/s]\n",
      "  0%|          | 7/25817 [00:00<06:39, 64.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1: train loss/word=0.3108, kl loss/word=0.0321, reconstruction loss/word=4.0666, ppl=1.3645, tag loss=13.9287s\n",
      "Evaluating batch ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25817/25817 [06:10<00:00, 69.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1: dev loss/word=5.7202, kl loss/word=0.0414, reconstruction loss/word=4.6131, ppl=304.9764, tag loss=12.67s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/13304 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ... Iteration: 2 Epsilon: 0.079135245945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13304/13304 [1:32:28<00:00,  2.40it/s]\n",
      "  0%|          | 7/25817 [00:00<06:47, 63.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2: train loss/word=0.3176, kl loss/word=0.0576, reconstruction loss/word=4.0661, ppl=1.3739, tag loss=15.2875s\n",
      "Evaluating batch ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25817/25817 [06:09<00:00, 69.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2: dev loss/word=5.8946, kl loss/word=0.0771, reconstruction loss/word=4.5660, ppl=363.0699, tag loss=14.88s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/13304 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ... Iteration: 3 Epsilon: 0.240493282516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13304/13304 [1:37:10<00:00,  2.28it/s]\n",
      "  0%|          | 7/25817 [00:00<06:46, 63.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3: train loss/word=0.3496, kl loss/word=0.0808, reconstruction loss/word=4.3932, ppl=1.4186, tag loss=17.8629s\n",
      "Evaluating batch ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25817/25817 [06:09<00:00, 69.78it/s]\n",
      "  0%|          | 0/13304 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3: dev loss/word=6.1584, kl loss/word=0.1127, reconstruction loss/word=4.5299, ppl=472.6801, tag loss=18.02s\n",
      "Training ... Iteration: 4 Epsilon: 0.521742721359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13304/13304 [1:45:21<00:00,  2.10it/s]\n",
      "  0%|          | 7/25817 [00:00<06:37, 64.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4: train loss/word=0.3920, kl loss/word=0.0936, reconstruction loss/word=4.9153, ppl=1.4800, tag loss=20.1474s\n",
      "Evaluating batch ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25817/25817 [06:09<00:00, 69.92it/s]\n",
      "  0%|          | 0/13304 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4: dev loss/word=6.4051, kl loss/word=0.1595, reconstruction loss/word=4.5832, ppl=604.9246, tag loss=19.77s\n",
      "Training ... Iteration: 5 Epsilon: 0.755245055665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13304/13304 [1:57:03<00:00,  1.89it/s]\n",
      "  0%|          | 6/25817 [00:00<07:14, 59.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5: train loss/word=0.4162, kl loss/word=0.1014, reconstruction loss/word=5.2154, ppl=1.5162, tag loss=21.4078s\n",
      "Evaluating batch ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25817/25817 [06:17<00:00, 68.34it/s]\n",
      "  0%|          | 0/13304 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5: dev loss/word=6.8197, kl loss/word=0.1818, reconstruction loss/word=4.7896, ppl=915.7017, tag loss=21.98s\n",
      "Training ... Iteration: 6 Epsilon: 0.856595388894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13304/13304 [1:58:46<00:00,  1.87it/s]\n",
      "  0%|          | 7/25817 [00:00<06:51, 62.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6: train loss/word=0.4198, kl loss/word=0.1046, reconstruction loss/word=5.2904, ppl=1.5217, tag loss=21.0838s\n",
      "Evaluating batch ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25817/25817 [06:19<00:00, 67.96it/s]\n",
      "  0%|          | 0/13304 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6: dev loss/word=6.8885, kl loss/word=0.1950, reconstruction loss/word=4.9372, ppl=980.9103, tag loss=20.88s\n",
      "Training ... Iteration: 7 Epsilon: 0.888102982863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13304/13304 [1:58:48<00:00,  1.87it/s] \n",
      "  0%|          | 7/25817 [00:00<07:05, 60.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7: train loss/word=0.4180, kl loss/word=0.1023, reconstruction loss/word=5.3051, ppl=1.5190, tag loss=20.4317s\n",
      "Evaluating batch ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25817/25817 [06:16<00:00, 68.54it/s]\n",
      "  0%|          | 0/13304 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7: dev loss/word=6.9935, kl loss/word=0.2157, reconstruction loss/word=4.9875, ppl=1089.4894, tag loss=21.29s\n",
      "Training ... Iteration: 8 Epsilon: 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13304/13304 [1:56:56<00:00,  1.90it/s]\n",
      "  0%|          | 6/25817 [00:00<07:16, 59.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 8: train loss/word=0.4137, kl loss/word=0.1003, reconstruction loss/word=5.3105, ppl=1.5125, tag loss=19.2809s\n",
      "Evaluating batch ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25817/25817 [06:16<00:00, 68.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 8: dev loss/word=6.7943, kl loss/word=0.1870, reconstruction loss/word=5.0366, ppl=892.7160, tag loss=18.68s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/13304 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ... Iteration: 9 Epsilon: 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13304/13304 [1:57:18<00:00,  1.89it/s]\n",
      "  0%|          | 7/25817 [00:00<06:55, 62.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 9: train loss/word=0.4038, kl loss/word=0.0906, reconstruction loss/word=5.3091, ppl=1.4976, tag loss=16.9326s\n",
      "Evaluating batch ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 20968/25817 [05:07<01:11, 68.26it/s]"
     ]
    }
   ],
   "source": [
    "print ('Using batch size of {}.'.format(BATCH_SIZE))\n",
    "\n",
    "epsilon = EPSILON_MIN\n",
    "kl_weight = KL_WEIGHT_START\n",
    "steps = 0\n",
    "strikes = 0\n",
    "last_dev_loss = np.inf\n",
    "for ITER in range(100):\n",
    "    # Perform training\n",
    "    random.shuffle(train)\n",
    "    \n",
    "    batches = [train[i:i + BATCH_SIZE] for i in range(0, len(train), BATCH_SIZE)]\n",
    "    \n",
    "    train_words, train_loss, train_kl_loss, train_reconstruct_loss, total_tag_loss = 0, 0.0, 0.0, 0.0, 0.0\n",
    "    start = time()\n",
    "    \n",
    "    print ('Training ... Iteration:', ITER, 'Epsilon:', epsilon)\n",
    "    for i, batch in enumerate(tqdm(batches)):\n",
    "        dy.renew_cg()\n",
    "        losses = []\n",
    "        for sent_id, sent in enumerate(batch):\n",
    "            if len(sent[1]) < 1 or len(sent[0]) < 3:\n",
    "                continue\n",
    "            kl_loss, softmax_loss, tag_loss = calc_loss(sent, epsilon)\n",
    "            #total_loss = dy.esum([kl_loss, softmax_loss, tag_loss])\n",
    "            #train_loss += total_loss.value()\n",
    "            \n",
    "            # Gradually increase KL-Divergence loss\n",
    "            if steps < 15000:\n",
    "                kl_weight = 1 / (1 + np.exp(-0.001 * steps + 5))\n",
    "            else:\n",
    "                kl_weight = 1.0\n",
    "                \n",
    "            losses.append(dy.esum([kl_weight * kl_loss, softmax_loss, tag_loss]))\n",
    "\n",
    "            # Record the KL loss and reconstruction loss separately help you monitor the training.\n",
    "            train_kl_loss += kl_loss.value()\n",
    "            train_reconstruct_loss += softmax_loss.value()\n",
    "            total_tag_loss += tag_loss.value()\n",
    "            \n",
    "            train_words += len(sent[0])\n",
    "        steps += 1\n",
    "   \n",
    "        # Batch update\n",
    "        batch_loss = dy.esum(losses)/BATCH_SIZE\n",
    "        train_loss += batch_loss.value()\n",
    "        batch_loss.backward()\n",
    "        trainer.update()\n",
    "        \n",
    "        \n",
    "        #total_loss.backward()\n",
    "        #trainer.update()\n",
    "        #if (sent_id + 1) % 1000 == 0:\n",
    "        #    print(\"--finished %r sentences\" % (sent_id + 1))\n",
    "\n",
    "    # Gradually increase KL-Divergence loss\n",
    "    if steps < 100000:\n",
    "        epsilon = .9 / (1 + np.exp(-0.0001 * steps + 5))\n",
    "    else:\n",
    "        epsilon = EPSILON_MAX\n",
    "        \n",
    "    #epsilon = min(EPSILON_MAX, epsilon + 0.05)\n",
    "    print(\"iter %r: train loss/word=%.4f, kl loss/word=%.4f, reconstruction loss/word=%.4f, ppl=%.4f, tag loss=%.4fs\" % (\n",
    "        ITER, train_loss / train_words, train_kl_loss / train_words, train_reconstruct_loss / train_words,\n",
    "        math.exp(train_loss / train_words), total_tag_loss / len(train)))\n",
    "\n",
    "    # Evaluate on dev set\n",
    "    dev_words, dev_loss, dev_kl_loss, dev_reconstruct_loss, dev_tag_loss = 0, 0.0, 0.0, 0.0, 0.0\n",
    "    start = time()\n",
    "    print ('Evaluating batch ... ')\n",
    "    for sent_id, sent in enumerate(tqdm(dev)):\n",
    "        dy.renew_cg()\n",
    "        if len(sent[1]) < 1 or len(sent[0]) < 3:\n",
    "                continue\n",
    "        kl_loss, softmax_loss, tag_loss = calc_loss(sent)\n",
    "\n",
    "        dev_kl_loss += kl_loss.value()\n",
    "        dev_reconstruct_loss += softmax_loss.value()\n",
    "        dev_tag_loss += tag_loss.value()\n",
    "        dev_loss += kl_loss.value() + softmax_loss.value() + tag_loss.value()\n",
    "\n",
    "        dev_words += len(sent[0])\n",
    "        trainer.update()\n",
    "\n",
    "    print(\"iter %r: dev loss/word=%.4f, kl loss/word=%.4f, reconstruction loss/word=%.4f, ppl=%.4f, tag loss=%.2fs\" % (\n",
    "        ITER, dev_loss / dev_words, dev_kl_loss / dev_words, dev_reconstruct_loss / dev_words,\n",
    "        math.exp(dev_loss / dev_words), dev_tag_loss / len(dev)))\n",
    "    if dev_loss > last_dev_loss and ITER > 9:\n",
    "        strikes += 1\n",
    "    else:\n",
    "        strikes = 0\n",
    "        last_dev_loss = dev_loss\n",
    "        model.save('tweet_tag_vae.best.weights')\n",
    "        \n",
    "    if strikes >= PATIENCE:\n",
    "        print ('Early stopping after {} iterations.')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('trained_vae_joint_multimodal.weights.x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.populate('trained_vae_joint_multimodal.weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hallucinate_tags(tweet):\n",
    "    dy.renew_cg()\n",
    "    \n",
    "    # Transduce all batch elements with an LSTM\n",
    "    src = tweet\n",
    "\n",
    "    # initialize the LSTM\n",
    "    init_state_src = lstm_encode.initial_state()\n",
    "\n",
    "    # get the output of the first LSTM\n",
    "    src_output = init_state_src.add_inputs([embed[x] for x in src])[-1].output()\n",
    "\n",
    "    # Now compute mean and standard deviation of source hidden state.\n",
    "    W_mu_tweet = dy.parameter(W_mu_tweet_p)\n",
    "    V_mu_tweet = dy.parameter(V_mu_tweet_p)\n",
    "    b_mu_tweet = dy.parameter(b_mu_tweet_p)\n",
    "\n",
    "    W_sig_tweet = dy.parameter(W_sig_tweet_p)\n",
    "    V_sig_tweet = dy.parameter(V_sig_tweet_p)\n",
    "    b_sig_tweet = dy.parameter(b_sig_tweet_p)\n",
    "    \n",
    "    # Compute tweet encoding\n",
    "    mu_tweet      = mlp(src_output, W_mu_tweet,  V_mu_tweet,  b_mu_tweet)\n",
    "    log_var_tweet = mlp(src_output, W_sig_tweet, V_sig_tweet, b_sig_tweet)\n",
    "    \n",
    "    #W_mu_tag = dy.parameter(W_mu_tag_p)\n",
    "    #V_mu_tag = dy.parameter(V_mu_tag_p)\n",
    "    #b_mu_tag = dy.parameter(b_mu_tag_p)\n",
    "\n",
    "    #W_sig_tag = dy.parameter(W_sig_tag_p)\n",
    "    #V_sig_tag = dy.parameter(V_sig_tag_p)\n",
    "    #b_sig_tag = dy.parameter(b_sig_tag_p)\n",
    "    \n",
    "    # Compute tag encoding\n",
    "    #tags_tensor = dy.sparse_inputTensor([tags], np.ones((len(tags),)), (NUM_TAGS,))\n",
    "    \n",
    "    #mu_tag      = dy.dropout(mlp(tags_tensor, W_mu_tag,  V_mu_tag,  b_mu_tag), DROPOUT)\n",
    "    #log_var_tag = dy.dropout(mlp(tags_tensor, W_sig_tag, V_sig_tag, b_sig_tag), DROPOUT)\n",
    "    \n",
    "    # Combine encodings for mean and diagonal covariance\n",
    "    W_mu = dy.parameter(W_mu_p)\n",
    "    b_mu = dy.parameter(b_mu_p)\n",
    "\n",
    "    W_sig = dy.parameter(W_sig_p)\n",
    "    b_sig = dy.parameter(b_sig_p)\n",
    "    \n",
    "    \n",
    "    mu_tag = dy.zeros(HIDDEN_DIM)\n",
    "    log_var_tag = dy.zeros(HIDDEN_DIM)\n",
    "    \n",
    "    mu      = dy.affine_transform([b_mu,  W_mu,  dy.concatenate([mu_tweet, mu_tag])])\n",
    "    log_var = dy.affine_transform([b_sig, W_sig, dy.concatenate([log_var_tweet, log_var_tag])])\n",
    "\n",
    "    # KL-Divergence loss computation\n",
    "    kl_loss = -0.5 * dy.sum_elems(1 + log_var - dy.pow(mu, dy.inputVector([2])) - dy.exp(log_var))\n",
    "\n",
    "    z = reparameterize(mu, log_var)\n",
    "\n",
    "    # now step through the output sentence\n",
    "    all_losses = []\n",
    "\n",
    "    #current_state = lstm_decode.initial_state().set_s([z, dy.tanh(z)])\n",
    "    #prev_word = src[0]\n",
    "    #W_sm = dy.parameter(W_tweet_softmax_p)\n",
    "    #b_sm = dy.parameter(b_tweet_softmax_p)\n",
    "\n",
    "    #for next_word in src[1:]:\n",
    "    #    # feed the current state into the\n",
    "    #    current_state = current_state.add_input(embed[prev_word])\n",
    "    #    output_embedding = current_state.output()\n",
    "\n",
    "    #    s = dy.affine_transform([b_sm, W_sm, output_embedding])\n",
    "    #    all_losses.append(dy.pickneglogsoftmax(s, next_word))\n",
    "\n",
    "    #    prev_word = next_word\n",
    "    \n",
    "    #softmax_loss = dy.esum(all_losses)\n",
    "\n",
    "    W_hidden = dy.parameter(W_hidden_p)\n",
    "    b_hidden = dy.parameter(b_hidden_p)\n",
    "    \n",
    "    W_out = dy.parameter(W_tag_output_p)\n",
    "    b_out = dy.parameter(b_tag_output_p)\n",
    "    \n",
    "    h = dy.tanh(b_hidden + W_hidden * z)\n",
    "    o = dy.logistic(b_out + W_out * h)\n",
    "    \n",
    "    tag_ranks = o.value()\n",
    "    \n",
    "    gen_tags = []\n",
    "    for i, p in enumerate(tag_ranks):\n",
    "        if random.random() < p:\n",
    "            gen_tags.append(i)\n",
    "                               \n",
    "    return gen_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['young',\n",
       "  ',',\n",
       "  'african',\n",
       "  'scientists',\n",
       "  'inspiring',\n",
       "  'next',\n",
       "  'peer',\n",
       "  'group',\n",
       "  'of',\n",
       "  'innovators',\n",
       "  '<UNK>',\n",
       "  ' ',\n",
       "  '<UNK>'],\n",
       " ['women', 'Diversity'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[idx_to_vocab[i] for i in train[1][0]], [idx_to_tag[i] for i in train[1][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['states',\n",
       "  'working',\n",
       "  'on',\n",
       "  'new',\n",
       "  'accounts',\n",
       "  'for',\n",
       "  'disabled',\n",
       "  'families',\n",
       "  ':',\n",
       "  'see',\n",
       "  '<UNK>',\n",
       "  'story',\n",
       "  '<UNK>'],\n",
       " ['batman'],\n",
       " ['NHS',\n",
       "  'quote',\n",
       "  'UFC',\n",
       "  'photographer',\n",
       "  'startups',\n",
       "  'UK',\n",
       "  'ausbiz',\n",
       "  'NATO',\n",
       "  'Destiny',\n",
       "  'Sabres'])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 1000\n",
    "pred = hallucinate_tags(train[idx][0])\n",
    "pred_args = np.argsort(pred)[::-1]\n",
    "[idx_to_vocab[i] for i in train[idx][0]], [idx_to_tag[i] for i in train[idx][1]], [idx_to_tag[i] for i in pred_args[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.152557373046875e-06, 0.001961648464202881)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[3239], pred[165]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hallucinate_tweet(given_tags):\n",
    "    dy.renew_cg()\n",
    "    \n",
    "    # Transduce all batch elements with an LSTM\n",
    "    tags = given_tags\n",
    "\n",
    "    # initialize the LSTM\n",
    "    #init_state_src = lstm_encode.initial_state()\n",
    "\n",
    "    # get the output of the first LSTM\n",
    "    #src_output = init_state_src.add_inputs([embed[x] for x in src])[-1].output()\n",
    "\n",
    "    # Now compute mean and standard deviation of source hidden state.\n",
    "    #W_mu_tweet = dy.parameter(W_mu_tweet_p)\n",
    "    #V_mu_tweet = dy.parameter(V_mu_tweet_p)\n",
    "    #b_mu_tweet = dy.parameter(b_mu_tweet_p)\n",
    "\n",
    "    #W_sig_tweet = dy.parameter(W_sig_tweet_p)\n",
    "    #V_sig_tweet = dy.parameter(V_sig_tweet_p)\n",
    "    #b_sig_tweet = dy.parameter(b_sig_tweet_p)\n",
    "    \n",
    "    # Compute tweet encoding\n",
    "    #mu_tweet      = mlp(src_output, W_mu_tweet,  V_mu_tweet,  b_mu_tweet)\n",
    "    #log_var_tweet = mlp(src_output, W_sig_tweet, V_sig_tweet, b_sig_tweet)\n",
    "    \n",
    "    W_mu_tag = dy.parameter(W_mu_tag_p)\n",
    "    V_mu_tag = dy.parameter(V_mu_tag_p)\n",
    "    b_mu_tag = dy.parameter(b_mu_tag_p)\n",
    "\n",
    "    W_sig_tag = dy.parameter(W_sig_tag_p)\n",
    "    V_sig_tag = dy.parameter(V_sig_tag_p)\n",
    "    b_sig_tag = dy.parameter(b_sig_tag_p)\n",
    "    \n",
    "    # Compute tag encoding\n",
    "    tags_tensor = dy.sparse_inputTensor([tags], np.ones((len(tags),)), (NUM_TAGS,))\n",
    "    \n",
    "    mu_tag      = dy.dropout(mlp(tags_tensor, W_mu_tag,  V_mu_tag,  b_mu_tag), DROPOUT)\n",
    "    log_var_tag = dy.dropout(mlp(tags_tensor, W_sig_tag, V_sig_tag, b_sig_tag), DROPOUT)\n",
    "    \n",
    "    # Combine encodings for mean and diagonal covariance\n",
    "    W_mu = dy.parameter(W_mu_p)\n",
    "    b_mu = dy.parameter(b_mu_p)\n",
    "\n",
    "    W_sig = dy.parameter(W_sig_p)\n",
    "    b_sig = dy.parameter(b_sig_p)\n",
    "    \n",
    "    mu_tweet = dy.zeros(HIDDEN_DIM)\n",
    "    log_var_tweet = dy.zeros(HIDDEN_DIM)\n",
    "    \n",
    "    mu      = dy.affine_transform([b_mu,  W_mu,  dy.concatenate([mu_tweet, mu_tag])])\n",
    "    log_var = dy.affine_transform([b_sig, W_sig, dy.concatenate([log_var_tweet, log_var_tag])])\n",
    "\n",
    "    # KL-Divergence loss computation\n",
    "    kl_loss = -0.5 * dy.sum_elems(1 + log_var - dy.pow(mu, dy.inputVector([2])) - dy.exp(log_var))\n",
    "\n",
    "    z = reparameterize(mu, log_var)\n",
    "\n",
    "    # now step through the output sentence\n",
    "    all_losses = []\n",
    "\n",
    "    current_state = lstm_decode.initial_state().set_s([z, dy.tanh(z)])\n",
    "    prev_word = vocab[START]\n",
    "    W_sm = dy.parameter(W_tweet_softmax_p)\n",
    "    b_sm = dy.parameter(b_tweet_softmax_p)\n",
    "\n",
    "    gen_tweet = []\n",
    "    for i in range(20):\n",
    "        # feed the current state into the\n",
    "        current_state = current_state.add_input(embed[prev_word])\n",
    "        output_embedding = current_state.output()\n",
    "\n",
    "        s = dy.affine_transform([b_sm, W_sm, output_embedding])\n",
    "        p = dy.softmax(s).npvalue()\n",
    "        next_word = np.random.choice(VOCAB_SIZE, p=p/p.sum())\n",
    "        gen_tweet.append(next_word)\n",
    "        prev_word = next_word\n",
    "                               \n",
    "    return gen_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['savings'],\n",
       " ['shanghai',\n",
       "  '<UNK>',\n",
       "  'rt',\n",
       "  'platforms',\n",
       "  'htt',\n",
       "  'surprised',\n",
       "  'board',\n",
       "  'get',\n",
       "  'days',\n",
       "  'set',\n",
       "  '&',\n",
       "  '5-star',\n",
       "  'rt',\n",
       "  'ed',\n",
       "  'rt',\n",
       "  '@justintrudeau',\n",
       "  'in',\n",
       "  'jumped',\n",
       "  'rt',\n",
       "  'bear'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 1000\n",
    "[idx_to_tag[i] for i in train[idx][1]], [idx_to_vocab[i] for i in hallucinate_tweet(train[idx][1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<S>',\n",
       "  'states',\n",
       "  'working',\n",
       "  'on',\n",
       "  'new',\n",
       "  'accounts',\n",
       "  'for',\n",
       "  'disabled',\n",
       "  'families',\n",
       "  ':',\n",
       "  'see',\n",
       "  '<UNK>',\n",
       "  'story',\n",
       "  '<UNK>',\n",
       "  '</S>'],\n",
       " ['Ankara'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 1000\n",
    "[idx_to_vocab[i] for i in train[idx][0]], [idx_to_tag[i] for i in hallucinate_tags(train[idx][0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
