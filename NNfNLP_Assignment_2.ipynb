{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import spacy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle\n",
    "\n",
    "#import dynet_config\n",
    "#dynet_config.set(mem=1024, random_seed=12345)\n",
    "#dynet_config.set_gpu()\n",
    "import dynet as dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading spaCy\")\n",
    "nlp = spacy.load('en')\n",
    "assert nlp.path is not None\n",
    "print ('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 52\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 300\n",
    "HOPS = 2\n",
    "UNK_THRESHOLD = 10000\n",
    "LEARNING_RATE = 0.01\n",
    "DROPOUT = 0.2\n",
    "NUM_HISTORIES = 5\n",
    "NUM_TAGS = 3883\n",
    "BATCH_SIZE = 16\n",
    "TEST_SAMPLE = 1000\n",
    "\n",
    "UNK = '<UNK>'\n",
    "START = '<S>'\n",
    "END = '</S>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format data and align user histories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is code that loads a preprocessed data so it does not need to recompute values if it needs to be restarted which can take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle('huang2016_train.pkl')\n",
    "dev_df = pd.read_pickle('huang2016_valid.pkl')\n",
    "test_df = pd.read_pickle('huang2016_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hist_df = pd.read_pickle('huang2016_user_histories_train.pkl')\n",
    "dev_hist_df = pd.read_pickle('huang2016_user_histories_valid.pkl')\n",
    "test_hist_df = pd.read_pickle('huang2016_user_histories_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def align_histories(data, histories):\n",
    "    aligned_texts = []\n",
    "    aligned_tags = []\n",
    "    aligned_histories = []\n",
    "\n",
    "    for idx, row in tqdm(data.iterrows(), total=len(data)):\n",
    "        hist = histories[histories['user_screen_name'] == row['user_screen_name']]\n",
    "        if len(hist) != NUM_HISTORIES:\n",
    "            continue\n",
    "        aligned_texts.append(row['text_no_tags'])\n",
    "        aligned_tags.append(row['tags'])\n",
    "        aligned_histories.append(list(hist['text_no_tags']))\n",
    "    \n",
    "    return aligned_texts, aligned_tags, list(zip(*aligned_histories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('huang2016_train.aligned.pkl', 'rb') as f:\n",
    "        twitter_texts, twitter_tags, twitter_histories = pickle.load(f)\n",
    "except:\n",
    "    twitter_texts, twitter_tags, twitter_histories = align_histories(train_df, train_hist_df)\n",
    "    with open('huang2016_train.aligned.pkl', 'wb') as f:\n",
    "        pickle.dump((twitter_texts, twitter_tags, twitter_histories), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('huang2016_valid.aligned.pkl', 'rb') as f:\n",
    "        dev_texts, dev_tags, dev_histories = pickle.load(f)\n",
    "except:\n",
    "    dev_texts, dev_tags, dev_histories = align_histories(dev_df, dev_hist_df)\n",
    "    with open('huang2016_valid.aligned.pkl', 'wb') as f:\n",
    "        pickle.dump((dev_texts, dev_tags, dev_histories), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('huang2016_test.aligned.pkl', 'rb') as f:\n",
    "        test_texts, test_tags, test_histories = pickle.load(f)\n",
    "except:\n",
    "    test_texts, test_tags, test_histories = align_histories(test_df, test_hist_df)\n",
    "    with open('huang2016_test.aligned.pkl', 'wb') as f:\n",
    "        pickle.dump((test_texts, test_tags, test_histories), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def index_tags(tags_list, tag_set, tag_dict):\n",
    "    return [[tag_dict[tag] for tag in tags if tag in tag_set] for tags in tags_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tag set\n",
    "tag_counts = defaultdict(int)\n",
    "for t in twitter_tags:\n",
    "    for x in t:\n",
    "        tag_counts[x] += 1\n",
    "top_k_tags = set(sorted(tag_counts, key=tag_counts.get, reverse=True)[:NUM_TAGS])\n",
    "\n",
    "tag_set = set()\n",
    "for t in twitter_tags:\n",
    "    tag_set.update(set([x for x in t if x in top_k_tags]))\n",
    "    \n",
    "tag_set = sorted(tag_set)\n",
    "print ('{} unique tags.'.format(len(tag_set)))\n",
    "#NUM_TAGS = len(tag_set)\n",
    "\n",
    "#tag_indices = dict((t, i) for i, t in enumerate(tag_set))\n",
    "#indices_tag = dict((i, t) for i, t in enumerate(tag_set))\n",
    "tag_indexes = defaultdict(lambda: len(tag_indexes))\n",
    "parsed_tags = index_tags(twitter_tags, tag_set, tag_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectroize tags (DON'T USE)\n",
    "\n",
    "twitter_y = np.zeros((len(twitter_tags), len(tag_set)), dtype=np.bool)\n",
    "for i, tags in tqdm(enumerate(twitter_tags), total=len(twitter_tags)):\n",
    "    for tag in tags:\n",
    "        twitter_y[i, tag_indices[tag]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print ('Attempting to open preprecessed TRAIN data ... ', end='')\n",
    "    t0=time()\n",
    "    with open('parsed_twitter_train_data.pkl', 'rb') as f:\n",
    "        vocab, parsed_texts, parsed_histories = pickle.load(f)\n",
    "    print ('DONE. ({:.3f}s)'.format(time()-t0))\n",
    "        \n",
    "except:\n",
    "    print ('FAIL.')\n",
    "    vocab = defaultdict(lambda: len(vocab))\n",
    "    print ('\\tParsing texts ... ', end='')\n",
    "    t0=time()\n",
    "    parsed_texts = [[vocab[str(w)] for w in t if not w.is_stop][:MAX_LEN] for t in nlp.pipe([x.encode('ascii', 'ignore').decode('ascii').lower() for x in twitter_texts], n_threads=3, batch_size=20000)]\n",
    "    print ('DONE. ({:.3f}s)'.format(time()-t0))\n",
    "\n",
    "    print ('\\tParsing histories ... ', end='')\n",
    "    t0=time()\n",
    "    parsed_histories = [[[vocab[str(w)] for w in t if not w.is_stop][:MAX_LEN] for t in nlp.pipe([x.encode('ascii', 'ignore').decode('ascii').lower() for x in h], n_threads=3, batch_size=20000)] for h in twitter_histories]\n",
    "    parsed_histories = list(zip(*parsed_histories))\n",
    "    print ('DONE. ({:.3f}s)'.format(time()-t0))\n",
    "    \n",
    "    unk_idx = vocab[UNK]\n",
    "    sos_idx = vocab[START]\n",
    "    eos_idx = vocab[END]\n",
    "    \n",
    "    print ('\\tSAVING parsed data ... ', end='')\n",
    "    t0=time()\n",
    "    with open('parsed_twitter_train_data.pkl', 'wb') as f:\n",
    "        pickle.dump((dict(vocab), parsed_texts, parsed_histories), f) \n",
    "    print ('DONE. ({:.3f}s)'.format(time()-t0))\n",
    "\n",
    "unk_idx = vocab[UNK]\n",
    "sos_idx = vocab[START]\n",
    "eos_idx = vocab[END]\n",
    "# Set unknown words to be UNK --> note as written, the paper does not indicate that any training data is labeled as UNK...\n",
    "vocab = defaultdict(lambda: unk_idx, vocab)\n",
    "idx_to_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "VOCAB_SIZE = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse dev and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print ('Attempting to open preprecessed DEV and TEST data ... ', end='')\n",
    "    t0=time()\n",
    "    with open('parsed_twitter_test_dev_data.pkl', 'rb') as f:\n",
    "        parsed_dev_texts, parsed_test_texts, parsed_dev_histories, parsed_test_histories = pickle.load(f)\n",
    "    print ('DONE. ({:.3f}s)'.format(time()-t0))\n",
    "        \n",
    "except:\n",
    "    print ('FAIL.')\n",
    "    print ('\\tParsing texts ... ', end='')\n",
    "    t0=time()\n",
    "    parsed_dev_texts = [[vocab[str(w)] for w in t if not w.is_stop][:MAX_LEN] for t in nlp.pipe([x.encode('ascii', 'ignore').decode('ascii').lower() for x in dev_texts], n_threads=3, batch_size=20000)]\n",
    "    parsed_test_texts = [[vocab[str(w)] for w in t if not w.is_stop][:MAX_LEN] for t in nlp.pipe([x.encode('ascii', 'ignore').decode('ascii').lower() for x in test_texts], n_threads=3, batch_size=20000)]\n",
    "    print ('DONE. ({:.3f}s)'.format(time()-t0))\n",
    "\n",
    "    print ('\\tParsing histories ... ', end='')\n",
    "    t0=time()\n",
    "    parsed_dev_histories = [[[vocab[str(w)] for w in t if not w.is_stop][:MAX_LEN] for t in nlp.pipe([x.encode('ascii', 'ignore').decode('ascii').lower() for x in h], n_threads=3, batch_size=20000)] for h in dev_histories]\n",
    "    parsed_test_histories = [[[vocab[str(w)] for w in t if not w.is_stop][:MAX_LEN] for t in nlp.pipe([x.encode('ascii', 'ignore').decode('ascii').lower() for x in h], n_threads=3, batch_size=20000)] for h in test_histories]\n",
    "    parsed_dev_histories = list(zip(*parsed_dev_histories))\n",
    "    parsed_test_histories = list(zip(*parsed_test_histories))\n",
    "    print ('DONE. ({:.3f}s)'.format(time()-t0))\n",
    "    \n",
    "    print ('\\tSAVING parsed data ... ', end='')\n",
    "    t0=time()\n",
    "    with open('parsed_twitter_test_dev_data.pkl', 'wb') as f:\n",
    "        pickle.dump((parsed_dev_texts, parsed_test_texts, parsed_dev_histories, parsed_test_histories), f) \n",
    "    print ('DONE. ({:.3f}s)'.format(time()-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dynet model\n",
    "model = dy.ParameterCollection()\n",
    "\n",
    "# The paper uses AdaGrad\n",
    "trainer = dy.AdagradTrainer(model, learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Embedding parameters\n",
    "EMBED_A = model.add_lookup_parameters((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "EMBED_B = model.add_lookup_parameters((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "EMBED_C = model.add_lookup_parameters((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "# User interest encoder parameters\n",
    "p_W_o = model.add_parameters((NUM_HISTORIES, EMBEDDING_DIM))\n",
    "p_W_s = model.add_parameters((NUM_HISTORIES, EMBEDDING_DIM))\n",
    "p_W_ms = model.add_parameters(NUM_HISTORIES)\n",
    "\n",
    "# Final prediction layer\n",
    "p_theta_s = model.add_parameters((NUM_TAGS, NUM_TAGS))\n",
    "p_W_f = model.add_parameters((NUM_TAGS, EMBEDDING_DIM*2))\n",
    "p_b_f = model.add_parameters(NUM_TAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_scores(tweet, histories, h=HOPS):\n",
    "    \n",
    "    \n",
    "    W_o     = dy.parameter(p_W_o)\n",
    "    W_s     = dy.parameter(p_W_s)\n",
    "    W_ms    = dy.parameter(p_W_ms)\n",
    "    W_f     = dy.parameter(p_W_f)\n",
    "    theta_s = dy.parameter(p_theta_s)\n",
    "    b_f     = dy.parameter(p_b_f)\n",
    "    \n",
    "    # Tweet encoder\n",
    "    w_emb = [dy.lookup(EMBED_A, x) for x in tweet]\n",
    "    o_0 = dy.esum(w_emb)\n",
    "    #print ('o_0:\\t', o_0.dim())\n",
    "    \n",
    "    o_h = o_0\n",
    "    \n",
    "    # User interest encoder\n",
    "    for i in range(h):\n",
    "\n",
    "        # Word-level attention\n",
    "        m_emb = [dy.concatenate([dy.lookup(EMBED_B, x) for x in t], 1) for t in histories]\n",
    "        #print ('m:\\t' ,[x.dim() for x in m_emb])\n",
    "        p = [dy.softmax(dy.transpose(o_h) * m, 1) for m in m_emb]\n",
    "        #print ('p:\\t' ,[x.dim() for x in p])\n",
    "\n",
    "        c_emb = [dy.concatenate([dy.lookup(EMBED_B, x) for x in t], 1) for t in histories]\n",
    "        #print ('c:\\t' ,[x.dim() for x in c_emb])\n",
    "        s = [dy.sum_dim(dy.cmult(p[i], c), [1], True) for i, c in enumerate(c_emb)]\n",
    "        #print ('s:\\t' , [x.dim() for x in s])\n",
    "\n",
    "        # Sentence-level attention\n",
    "        m_s = [dy.tanh(W_o * o_h + W_s * s_i) for s_i in s]\n",
    "        #print ('m_s:\\t' , [x.dim() for x in m_s])\n",
    "\n",
    "        p_s = [dy.softmax(dy.transpose(W_ms) * m, 0) for m in m_s]\n",
    "        #print ('p_s:\\t' , [x.dim() for x in p_s])\n",
    "\n",
    "        u = dy.esum([dy.cmult(p_s[i], s_i) for i, s_i in enumerate(s)])\n",
    "        #print ('u:\\t' , u.dim())\n",
    "        \n",
    "        o_h = o_h + u\n",
    "        \n",
    "    # Final prediction\n",
    "    out = dy.softmax(dy.transpose(theta_s) * (W_f * dy.concatenate([o_0, o_h]) + b_f))\n",
    "    #out = dy.transpose(theta_s) * (W_f * dy.concatenate([o_0, o_h]) + b_f)\n",
    "    #print ('out:\\t', out.dim())\n",
    "    \n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list(zip(parsed_texts, parsed_tags, parsed_histories))\n",
    "dev_tags = index_tags(dev_tags, tag_set, tag_indexes)\n",
    "dev = list(zip(parsed_dev_texts, dev_tags, parsed_dev_histories))\n",
    "\n",
    "print ('Using batch size of {}.'.format(BATCH_SIZE))\n",
    "\n",
    "for ITER in range(60):\n",
    "    # Train\n",
    "    random.shuffle(train)\n",
    "    \n",
    "    batches = [train[i:i + BATCH_SIZE] for i in range(0, len(train), BATCH_SIZE)]    \n",
    "    \n",
    "    for i, batch in enumerate(tqdm(batches, total=len(batches))):\n",
    "        dy.renew_cg()\n",
    "        train_loss = 0.0\n",
    "        t0 = time()\n",
    "\n",
    "        losses =[]\n",
    "\n",
    "        for words, tags, hists in batch:\n",
    "            if len(tags) == 0 or len(hists) == 0 or len(words) == 0 or not all(len(h) > 0 for h in hists):\n",
    "                continue\n",
    "            scores = calc_scores(words, hists)\n",
    "            loss = dy.esum([-dy.log(scores[tag]) for tag in tags])\n",
    "            losses.append(loss)\n",
    "\n",
    "        # Batch update\n",
    "        batch_loss = dy.esum(losses)/BATCH_SIZE\n",
    "        batch_loss.backward()\n",
    "        trainer.update()\n",
    "        \n",
    "        # Do frequent tests to get a loss graph\n",
    "        if i == 0 or (i % 100) == 0:\n",
    "            tt = time()\n",
    "            dev_loss = 0.\n",
    "            for words, tags, hists in random.sample(dev, TEST_SAMPLE):\n",
    "                dy.renew_cg()\n",
    "                if len(tags) == 0 or len(hists) == 0 or len(words) == 0 or not all(len(h) > 0 for h in hists):\n",
    "                    continue\n",
    "                scores = calc_scores(words, hists)\n",
    "                loss = dy.esum([-dy.log(scores[tag]) for tag in tags])\n",
    "                dev_loss += loss.value()\n",
    "            print(\"val_loss={:.4f}, time={:.3f}\".format(dev_loss / TEST_SAMPLE, time()-tt))\n",
    "    print(\"Iteration %r: train loss/tweet=%.4f, time=%.2fs\" % (ITER, train_loss / len(train), time() - t0))\n",
    "    \n",
    "    dev_loss = 0.\n",
    "    for words, tags, hists in tqdm(random.sample(dev, TEST_SAMPLE), total=TEST_SAMPLE):\n",
    "        dy.renew_cg()\n",
    "        if len(tags) == 0 or len(hists) == 0 or len(words) == 0 or not all(len(h) > 0 for h in hists):\n",
    "            continue\n",
    "        scores = calc_scores(words, hists)\n",
    "        loss = dy.esum([-dy.log(scores[tag]) for tag in tags])\n",
    "        dev_loss += loss.value()\n",
    "    print(\"Iteration %r: val loss=%.4f\" % (ITER, dev_loss / len(dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"sota.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.populate('sota.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
